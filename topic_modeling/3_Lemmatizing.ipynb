{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing files using spaCy\n",
    "\n",
    "In this notebook, we will lemmatize our corpus. This needs to be done for each language separately. Lemmatizing is not obligatory for Topic Modeling, but if your lemmatization model works well with your corpus, we recommend it, since this can improve the quality of the topics.<br>  \n",
    "<i>spaCy</i> is a python library for natural language processing. See more: https://spacy.io/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from cophi_toolbox import preprocessing\n",
    "import metadata_toolbox.utils as metadata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Y:/data/projekte/dispecs/topicModeling'\n",
    "language = 'fr' # language 2 letter abbreviation\n",
    "path_to_corpus = Path(data, 'dispecs_'+language+'_lemmatized') # Careful! The files will be overwritten, so make a backup :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '{year}_{journal}_{author}_{volume}_{issue}_{id}'#1716_Le-Spectateur-ou-le-Socrate-moderne_Anonym_Table-des-Matieres_119-1257\n",
    "meta = pd.concat([metadata.fname2metadata(str(path), pattern=pattern) for path in path_to_corpus.glob('*.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>journal</th>\n",
       "      <th>author</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Y:\\data\\projekte\\dispecs\\topicModeling\\dispecs_fr_lemmatized_test\\1711-1712_Le-Misantrope_Justus-Van-Effen_Vol-1_Nr-001_2948.txt</th>\n",
       "      <td>1711-1712</td>\n",
       "      <td>Le-Misantrope</td>\n",
       "      <td>Justus-Van-Effen</td>\n",
       "      <td>Vol-1</td>\n",
       "      <td>Nr-001</td>\n",
       "      <td>2948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         year        journal  \\\n",
       "Y:\\data\\projekte\\dispecs\\topicModeling\\dispecs_...  1711-1712  Le-Misantrope   \n",
       "\n",
       "                                                              author volume  \\\n",
       "Y:\\data\\projekte\\dispecs\\topicModeling\\dispecs_...  Justus-Van-Effen  Vol-1   \n",
       "\n",
       "                                                     issue    id  \n",
       "Y:\\data\\projekte\\dispecs\\topicModeling\\dispecs_...  Nr-001  2948  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write your own dictionaries for lemmatization of special cases.\n",
    "The usage of upper and lowercase letters in values is relevant, so be sure to correct both versions, if needed.\n",
    "\n",
    "\"\"\"\n",
    "corr_fr = {\n",
    "    \"avoir\" : [\"avois\", \"avoit\", \"Avois\", \"Avoit\"], \n",
    "    \"dire\" : [\"disois\", \"disoit\", \"Disois\", \"Disoit\"],\n",
    "    \"manière\" : [\"maniere\", \"Maniere\"],\n",
    "    \"pièce\" : [\"piéce\", \"Piéce\"],\n",
    "    \"poète\" : [\"poëte\", \"Poëte\", \"Poëtes\", \"poëtes\"],\n",
    "    \"poème\" : [\"poëme\", \"Poëme\"],\n",
    "    \"poésie\" : [\"poësie\", \"Poësie\"],\n",
    "    \"sexe\" : [\"séxe\", \"Séxe\"],\n",
    "    \"moyen\" : [\"moïen\", \"Moïen\"],\n",
    "    \"thèatre\":[\"théâtre\", \"Théâtre\", \"théatre\",\"Théatre\"],\n",
    "    \"comédie\":[\"comédien\",\"Comédien\"],\n",
    "    \"tragédie\":[\"tragedie\",\"Tragedie\"],\n",
    "    \"société\":[\"societe\", \"societé\",\"Societe\", \"Societé\"],\n",
    "    \"feuille\":[\"feüille\",\"Feüille\"]\n",
    "        \n",
    "}\n",
    "\n",
    "corr_es = {\n",
    "    \"decir\":[\"dixo\", \"decia\", \"Dixo\", \"Decia\"],\n",
    "    \"ir\":[\"iba\", \"Iba\"],\n",
    "    \"pacerer\":[\"parecia\", \"Parecia\"],\n",
    "    \"poder\":[\"podia\", \"Podia\"],\n",
    "    \"ser\":[\"fuesse\", \"Fuesse\"],\n",
    "    \"haber\":[\"habia\", \"havia\", \"Habia\", \"Havia\"],\n",
    "    \"ahora\" : [\"aora\", \"Aora\"],\n",
    "    \"estar\" : [\"estàn\", \"Estàn\"],\n",
    "    \"lujo\" : [\"luxo\",\"luxar\", \"Luxo\",\"Luxar\"],\n",
    "    \"razón\" : [\"razon\", \"razòn\", \"Razon\", \"Razòn\"],\n",
    "    \"caballero\" : [\"cavallero\", \"Cavallero\"],\n",
    "    \"mujer\" : [\"muger\", \"mugeres\", \"Muger\", \"Mugeres\"],\n",
    "    \"vez\" : [\"vèz\", \"Vèz\"],\n",
    "    \"jamás\" : [\"jamas\", \"Jamas\"],\n",
    "    \"demás\" : [\"demas\", \"demàs\", \"Demas\", \"Demàs\"],\n",
    "    \"cuidar\" : [\"cuydado\", \"Cuydado\"],\n",
    "    \"posible\" : [\"possible\", \"Possible\"],\n",
    "    \"comedia\":[\"comediar\", \"Comedias\"],\n",
    "    \"poeta\":[\"poetas\", \"Poetas\"],\n",
    "    \"mano\":[\"manir\", \"Manir\"],\n",
    "    \"barba\":[\"barbar\", \"Barbar\"],\n",
    "    \"idea\":[\"ideo\", \"Ideo\"],\n",
    "    \"nada\":[\"nadar\", \"Nadar\"]\n",
    "}\n",
    "\n",
    "corr_none={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French package loaded. Corrections dictionary for French defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the language packages and special lemmatization rules defined in the dictionaries above.\n",
    "\n",
    "-----> Language packages (have to be installed first, see here: https://spacy.io/usage/models):\n",
    "French: fr_core_news_lg\n",
    "Spanish: es_core_news_lg\n",
    "Italian: it_core_news_lg\n",
    "English: en_core_web_lg\n",
    "Portuguese: pt_core_news_lg\n",
    "German: de_core_news_lg\n",
    "\"\"\"\n",
    "if language == 'fr':\n",
    "    nlp = spacy.load('fr_core_news_lg')\n",
    "    correction_dictionary=corr_fr\n",
    "    print('French package loaded. Corrections dictionary for French defined.')\n",
    "if language == 'it':\n",
    "    nlp = spacy.load('it_core_news_lg')\n",
    "    correction_dictionary=corr_none\n",
    "    print('Italian package loaded. Correction dictionary is empty.')\n",
    "if language == 'es':\n",
    "    nlp = spacy.load('es_core_news_lg')\n",
    "    correction_dictionary=corr_es\n",
    "    print('Spanish package loaded. Corrections dictionary for Spanish defined.')\n",
    "if language == 'de':\n",
    "    nlp = spacy.load('de_core_news_lg')\n",
    "    correction_dictionary=corr_none\n",
    "    print('German package loaded. Correction dictionary is empty.')\n",
    "if language == 'en':\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    correction_dictionary=corr_none\n",
    "    print('English package loaded. Correction dictionary is empty.')\n",
    "if language == 'pt':\n",
    "    nlp = spacy.load('pt_core_news_lg')\n",
    "    correction_dictionary=corr_none\n",
    "    print('Portuguese package loaded. Correction dictionary is empty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize tokenizer to not tokenize the paragraph marker ('###')\n",
    "from spacy.tokenizer import Tokenizer\n",
    "special_cases = {\"###\": [{\"ORTH\": \"###\"}]}\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases)\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization of all text files in the corpus. The files will be overwritten. \n",
    "\n",
    "for file in path_to_corpus.glob('*.txt'): \n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        # replace characters from first rule with None, from second rule with whitespace\n",
    "        original = f.read().translate(str.maketrans('', '', '.,;!?$:¡¿()\\\"\\“'))\n",
    "#         lemmatized_object = nlp(original)        \n",
    "#         lemma_list = []\n",
    "#         for lemma in lemmatized_object:\n",
    "#             lemma_list.append(lemma.lemma_)\n",
    "#         lemma_doc = ' '.join(lemma_list)\n",
    "        lemmatized_object = nlp(original)\n",
    "        lemma_list = []\n",
    "        for lemma in lemmatized_object:\n",
    "            lemma=(lemma.lemma_).translate(str.maketrans('’\\'', '  ')).lower()\n",
    "            for key, value in correction_dictionary.items():\n",
    "                if lemma in value: #.translate(None, '.,;!?$:¡¿()\\\"')\n",
    "                    #print(lemma+\" replaced with \"+key)\n",
    "                    lemma=key\n",
    "            lemma_list.append(lemma)\n",
    "            #print(lemma+\" appended to list\")\n",
    "        lemma_doc = ' '.join(lemma_list)\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        f.write(lemma_doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
