{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics â€“ Easy Topic Modeling in Python\n",
    "\n",
    "The text mining technique **Topic Modeling** has become a popular statistical method for clustering documents. This [Jupyter notebook](http://jupyter.org/) introduces a step-by-step workflow, basically containing data preprocessing, the actual topic modeling using **latent Dirichlet allocation** (LDA), which learns the relationships between words, topics, and documents, as well as some interactive visualizations to explore the model.\n",
    "\n",
    "LDA, introduced in the context of text analysis in [2003](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), is an instance of a more general class of models called **mixed-membership models**. Involving a number of distributions and parameters, the topic model is typically performed using [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) with conjugate priors and is purely based on word frequencies.\n",
    "\n",
    "There have been written numerous introductions to topic modeling for humanists (e.g. [this one](http://scottbot.net/topic-modeling-for-humanists-a-guided-tour/)), which provide another level of detail regarding its technical and epistemic properties.\n",
    "\n",
    "For this workflow, you will need a corpus (a set of texts) as plain text (`.txt`) or [TEI XML](http://www.tei-c.org/index.xml) (`.xml`). Using the `dariah_topics` package, you also have the ability to process the output of [DARIAH-DKPro-Wrapper](https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper), a command-line tool for *natural language processing*.\n",
    "\n",
    "Topic modeling works best with very large corpora. The [TextGrid Repository](https://textgridrep.org/) is a great place to start searching for text data. Anyway, to demonstrate the technique, we provide one small text collection in the folder `grenzboten_sample` containing 15 diary excerpts, as well as 15 war diary excerpts, which appeared in *Die Grenzboten*, a German newspaper of the late 19th and early 20th century.\n",
    "\n",
    "**Of course, you can work with your own corpus in this notebook.**\n",
    "\n",
    "We're relying on the LDA implementation by [Allen B. Riddell](https://www.ariddell.org/), called [lda](http://pythonhosted.org/lda/index.html), which is very lightweight. Aside from that, we provide two more Jupyter notebooks:\n",
    "\n",
    "* [IntroducingMallet](IntroducingMallet.ipynb), using LDA by [MALLET](http://mallet.cs.umass.edu/topics.php), which is known to be very robust. \n",
    "* [IntroducingGensim](IntroducingGensim.ipynb), using LDA by called [Gensim](https://radimrehurek.com/project/gensim/), which is attractive because of its multi-core support.\n",
    "\n",
    "For more information in general, have a look at the [documentation](http://dev.digital-humanities.de/ci/job/DARIAH-Topics/doclinks/1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: Installing dependencies\n",
    "\n",
    "To work within this Jupyter notebook, you will have to import the `dariah_topics` library. As you do, `dariah_topics` also imports a couple of external libraries, which have to be installed first. `pip` is the preferred installer program in Python. Starting with Python 3.4, it is included by default with the Python binary installers. If you are interested in `pip`, have a look at [this website](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "To install the `dariah_topics` library with all dependencies, open your commandline, go with `cd` to the folder `Topics` and run:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Alternatively, you can do:\n",
    "\n",
    "```\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "If you get any errors or are not able to install *all* dependencies properly, try [Stack Overflow](https://stackoverflow.com/questions/tagged/pip) for troubleshooting or create a new issue on our [GitHub page](https://github.com/DARIAH-DE/Topics).\n",
    "\n",
    "**Important**: If you are on macOS or Linux, you will have to use `pip3` and `python3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final words\n",
    "As you probably already know, code has to be written in the grey cells. You execute a cell by clicking the **Run**-button (or **Ctrl + Enter**). If you want to run all cells of the notebook at once, click **Cell > Run All** or **Kernel > Restart & Run All** respectively, if you want to restart the Python kernel first. On the left side of an (unexecuted) cell stands `In [ ]:`. The empty bracket means, that the cell hasn't been executed yet. By clicking **Run**, a star appears in the brackets (`In [*]:`), which means the process is running. In most cases, you won't see that star, because your computer is faster than your eyes. You can execute only one cell at once, all following executions will be in the waiting line. If the process of a cell is done, a number appears in the brackets (`In [1]:`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Starting with topic modeling!\n",
    "\n",
    "Execute the following cell to import modules from the `dariah_topics` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cophi_toolbox import preprocessing\n",
    "from dariah_topics import postprocessing\n",
    "from dariah_topics import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module dariah_topics.postprocessing in dariah_topics:\n",
      "\n",
      "NAME\n",
      "    dariah_topics.postprocessing\n",
      "\n",
      "DESCRIPTION\n",
      "    Postprocessing Text Data, Saving Matrices, Corpora and LDA Models\n",
      "    *****************************************************************\n",
      "    \n",
      "    Functions of this module are for **postprocessing purpose**. You can save `document-term matrices <https://en.wikipedia.org/wiki/Document-term_matrix>`_, `tokenized corpora <https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)>`_ and `LDA models <https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>`_, access topics, topic probabilites for documents, and word probabilities for each topic. All matrix variants provided in :func:`preprocessing.create_document_term_matrix()`_ are supported, as well as `lda <https://pypi.python.org/pypi/lda>`_, `Gensim <https://radimrehurek.com/gensim/>`_ and `MALLET <http://mallet.cs.umass.edu/topics.php>`_ models or output, respectively. Recurrent variable names are based on the following conventions:\n",
      "    \n",
      "        * ``topics`` means a pandas DataFrame containing the top words for each     topic and any Dirichlet parameters.\n",
      "        * ``document_topics`` means a pandas DataFrame containing topic proportions per     document, at the end of the iterations.\n",
      "        * ``word_weights`` means unnormalized weights for every topic and word type.\n",
      "        * ``keys`` means the top *n* tokens of a topic.\n",
      "    \n",
      "    Contents\n",
      "    ********\n",
      "        * :func:`doc2bow()`\n",
      "        * :func:`save_document_term_matrix()` writes a document-term matrix to a `CSV <https://en.wikipedia.org/wiki/Comma-separated_values>`_\n",
      "        file or to a `Matrix Market <http://math.nist.gov/MatrixMarket/formats.html#MMformat>`_ file, respectively.\n",
      "        * :func:`save_model()` saves a LDA model (except MALLET models, which will be saved     by specifying a parameter of :func:`mallet.create_mallet_model()`).\n",
      "        * :func:`save_tokenized_corpus()` writes tokens of a tokenized corpus to plain text     files per document.\n",
      "        * :func:`show_document_topics()` shows topic probabilities for each document.\n",
      "        * :func:`show_topics()` shows topics generated by a LDA model.\n",
      "        * :func:`show_word_weights()` shows word probabilities for each topic.\n",
      "\n",
      "FUNCTIONS\n",
      "    doc2bow(document_term_matrix)\n",
      "        Creates a `doc2bow` pandas Series for Gensim.\n",
      "        \n",
      "        With this function you can create a `doc2bow` pandas Series as input for Gensim, e.g.     to instantiate the :class:`gensim.models.LdaModel` class or get topic distributions     with :func:`gensim.models.LdaModel.get_document_topics()`.\n",
      "        \n",
      "        Args:\n",
      "            document_term_matrix (pandas.DataFrame): A document-term matrix **designed\n",
      "                for large corpora**.\n",
      "        \n",
      "        Returns:\n",
      "            List of lists containing tuples.\n",
      "        \n",
      "        Todo:\n",
      "            * Improve efficiency.\n",
      "        \n",
      "        Example:\n",
      "            >>> from cophi_toolbox import preprocessing\n",
      "            >>> tokenized_corpus = [['this', 'is', 'document', 'one'], ['this', 'is', 'document', 'two']]\n",
      "            >>> document_labels = ['document_one', 'document_two']\n",
      "            >>> document_term_matrix, _, _ = preprocessing.create_document_term_matrix(tokenized_corpus, document_labels, True)\n",
      "            >>> isinstance(doc2bow(document_term_matrix), pd.Series)\n",
      "            True\n",
      "    \n",
      "    get_sorted_values_from_distribution(values, distribution, length)\n",
      "    \n",
      "    save_document_term_matrix(document_term_matrix, path, document_ids=None, type_ids=None, matrix_market=False)\n",
      "        Saves document-term matrix.\n",
      "        \n",
      "        Writes a ``document_term_matrix`` and, in case of a large corpus matrix,     ``document_ids`` and ``type_ids``, which have to be specified, to comma-separated     values (CSV) files. Furthermore, if ``document_term_matrix`` is designed for     large corpora and ``matrix_market`` is True, the matrix will be saved in the     `Matrix Market format <http://math.nist.gov/MatrixMarket/formats.html#MMformat>`_ (`.mm`).     Libraries like `scipy <https://www.scipy.org>`_ and `gensim <https://radimrehurek.com/gensim/>`_     are able to read and process the Matrix Market format.\n",
      "        Use the function :func:`preprocessing.create_document_term_matrix()` to create a\n",
      "        document-term matrix.\n",
      "        \n",
      "        Args:\n",
      "            document_term_matrix (pandas.DataFrame): Document-term matrix with rows\n",
      "                corresponding to ``document_labels`` and columns corresponding to types\n",
      "                (unique tokens in the corpus). The single values of the matrix are\n",
      "                type frequencies. Will be saved as ``document_term_matrix.csv`` or\n",
      "                ``document_term_matrix.mm``, respectively.\n",
      "            path (str): Path to the output directory.\n",
      "            document_ids (dict, optional): Dictionary containing ``document_labels`` as\n",
      "                keys and an unique identifier as value. Only required, if\n",
      "                ``document_term_matrix`` is designed for large corpora. Will be saved\n",
      "                as ``document_ids.csv``. Defaults to None.\n",
      "            type_ids (dict, optional): Dictionary containing types as keys and an\n",
      "                unique identifier as value. Only required, if ``document_term_matrix``\n",
      "                is designed for large corpora. Will be saved as ``type_ids.csv``. Defaults\n",
      "                to None.\n",
      "            matrix_market (bool, optional): If True, matrix will be saved in Matrix\n",
      "                Market format. Only for the large corpus variant of ``document_term_matrix``\n",
      "                available. Defaults to False.\n",
      "        \n",
      "        Returns:\n",
      "            None.\n",
      "        \n",
      "        Example:\n",
      "            >>> from cophi_toolbox import preprocessing\n",
      "            >>> import os\n",
      "            >>> path = 'tmp'\n",
      "            >>> tokenized_corpus = [['this', 'is', 'document', 'one'], ['this', 'is', 'document', 'two']]\n",
      "            >>> document_labels = ['document_one', 'document_two']\n",
      "            >>> document_term_matrix = preprocessing.create_document_term_matrix(tokenized_corpus, document_labels)\n",
      "            >>> save_document_term_matrix(document_term_matrix=document_term_matrix, path=path)\n",
      "            >>> preprocessing.read_document_term_matrix(os.path.join(path, 'document_term_matrix.csv')) #doctest +NORMALIZE_WHITESPACE\n",
      "                          this   is  document  two  one\n",
      "            document_one   1.0  1.0       1.0  0.0  1.0\n",
      "            document_two   1.0  1.0       1.0  1.0  0.0\n",
      "            >>> document_term_matrix, document_ids, type_ids = preprocessing.create_document_term_matrix(tokenized_corpus, document_labels, True)\n",
      "            >>> save_document_term_matrix(document_term_matrix, path, document_ids, type_ids)\n",
      "            >>> isinstance(preprocessing.read_document_term_matrix(os.path.join(path, 'document_term_matrix.csv')), pd.DataFrame)\n",
      "            True\n",
      "    \n",
      "    save_model(model, filepath)\n",
      "        Saves a LDA model.\n",
      "        \n",
      "        With this function you can save a LDA model using :module:`pickle`. If you want     to save MALLET models, you have to specify a parameter of the function :func:`mallet.create_mallet_model()`.\n",
      "        \n",
      "        Args:\n",
      "            model: Fitted LDA model produced by `Gensim <https://radimrehurek.com/gensim/>`_\n",
      "                or `lda <https://pypi.python.org/pypi/lda>`_.\n",
      "            filepath (str): Path to LDA model, e.g. ``/home/models/model.pickle``.\n",
      "        \n",
      "        Returns:\n",
      "            None.\n",
      "        \n",
      "        Example:\n",
      "            >>> from lda import LDA\n",
      "            >>> from gensim.models import LdaModel\n",
      "            >>> from dariah_topics import preprocessing\n",
      "            >>> save_model(LDA, 'model.pickle')\n",
      "            >>> preprocessing.read_model('model.pickle') == LDA\n",
      "            True\n",
      "            >>> save_model(LdaModel, 'model.pickle')\n",
      "            >>> preprocessing.read_model('model.pickle') == LdaModel\n",
      "            True\n",
      "    \n",
      "    save_tokenized_corpus(tokenized_corpus, document_labels, path)\n",
      "        Writes a tokenized corpus to text files.\n",
      "        \n",
      "        With this function you can write tokens of a `tokenized_corpus` to plain text     files per document to ``path``. Every file will be named after its ``document_label``.     Depending on the used tokenizer, ``tokenized_corpus`` does normally not contain     any punctuations or one-letter words.\n",
      "        Use the function :func:`preprocessing.tokenize()` to tokenize a corpus.\n",
      "        \n",
      "        Args:\n",
      "            tokenized_corpus (list): Tokenized corpus containing one or more\n",
      "                iterables containing tokens.\n",
      "            document_labels (list): Name of each `tokenized_document` in `tokenized_corpus`.\n",
      "            path (str): Path to the output directory.\n",
      "        \n",
      "        Returns:\n",
      "            None\n",
      "        \n",
      "        Example:\n",
      "            >>> tokenized_corpus = [['this', 'is', 'a', 'tokenized', 'document']]\n",
      "            >>> document_labels = ['document_label']\n",
      "            >>> path = 'tmp'\n",
      "            >>> save_tokenized_corpus(tokenized_corpus, document_labels, path)\n",
      "            >>> with open(os.path.join(path, 'document_label.txt'), 'r', encoding='utf-8') as file:\n",
      "            ...     file.read()\n",
      "            'this\\nis\\na\\ntokenized\\ndocument'\n",
      "    \n",
      "    show_document_topics(topics, model=None, document_labels=None, doc_topics_file=None, doc2bow=None, num_keys=3, easy_file_format=True, dec=4)\n",
      "        Shows topic distribution for each document.\n",
      "        \n",
      "        With this function you can show the topic distributions for all documents in a pandas DataFrame.     For each topic, the top ``num_keys`` keys will be considered. If you have a\n",
      "        * `lda <https://pypi.python.org/pypi/lda>`_ model, you have to pass the model     as ``model`` and the document-term matrix vocabulary as ``vocabulary``.\n",
      "        * `Gensim <https://radimrehurek.com/gensim/>`_ model, you have to pass only the model     as ``model``.\n",
      "        * `MALLET <http://mallet.cs.umass.edu/topics.php>`_ based workflow, you have to    pass only the ``doc_topics_file``.\n",
      "        \n",
      "        Args:\n",
      "            topics (pandas.DataFrame, optional): Only for lda models. A pandas DataFrame\n",
      "                containing all topics.\n",
      "            model (optional): lda or Gensim model.\n",
      "            document_labels (list, optional): An list of all document labels.\n",
      "            doc_topics_file (str, optional): Only for MALLET. Path to the doc-topics file.\n",
      "            doc2bow (list, optional): A list of lists containing tuples of ``type_id`` and\n",
      "                frequency.\n",
      "            num_keys (int, optional): Number of top keys for each topic.\n",
      "            dec (int, optional): Number of decimal places in the document-topics-value\n",
      "        \n",
      "        Returns:\n",
      "            A pandas DataFrame with rows corresponding to topics and columns corresponding\n",
      "                to keys.\n",
      "        \n",
      "        Example:\n",
      "    \n",
      "    show_topic_key_weights(topic_no, num_keys, model=None, vocabulary=None, topic_word_weights_file=None, sort_ascending=None)\n",
      "    \n",
      "    show_topics(model=None, vocabulary=None, topic_keys_file=None, num_keys=10)\n",
      "        Shows topics of LDA model.\n",
      "        \n",
      "        With this function you can show all topics of a LDA model in a pandas DataFrame.     For each topic, the top ``num_keys`` keys will be considered. If you have a\n",
      "        * `lda <https://pypi.python.org/pypi/lda>`_ model, you have to pass the model     as ``model`` and the document-term matrix vocabulary as ``vocabulary``.\n",
      "        * `Gensim <https://radimrehurek.com/gensim/>`_ model, you have to pass only the model     as ``model``.\n",
      "        * `MALLET <http://mallet.cs.umass.edu/topics.php>`_ based workflow, you have to    pass only the ``topic_keys_file``.\n",
      "        \n",
      "        Args:\n",
      "            model (optional): lda or Gensim model.\n",
      "            vocabulary (list, optional): Only for lda. The vocabulary of the \n",
      "                document-term matrix.\n",
      "            topic_keys_file (str): Only for MALLET. Path to the topic keys file.\n",
      "            num_keys (int, optional): Number of top keys for each topic. \n",
      "        \n",
      "        Returns:\n",
      "            A pandas DataFrame with rows corresponding to topics and columns corresponding\n",
      "                to keys.\n",
      "        \n",
      "        Example:\n",
      "    \n",
      "    show_word_weights(word_weights_file, num_tokens)\n",
      "        Read Mallet word_weigths file\n",
      "        \n",
      "        Description:\n",
      "            Reads Mallet word_weigths into pandas DataFrame.\n",
      "        \n",
      "        Args:\n",
      "            word_weigts_file: Word_weights_file created with Mallet\n",
      "        \n",
      "        Returns: Pandas DataFrame\n",
      "        \n",
      "        Todo:\n",
      "            * Adapt for ``lda`` and ``gensim`` output.\n",
      "        \n",
      "        Example:\n",
      "            >>> import tempfile\n",
      "            >>> with tempfile.NamedTemporaryFile(suffix='.txt') as tmpfile:\n",
      "            ...     tmpfile.write(b'0\\tthis\\t0.5\\n0\\tis\\t0.4\\n0\\ta\\t0.3\\n0\\tdocument\\t0.2') and True\n",
      "            ...     tmpfile.flush()\n",
      "            ...     show_word_weights(tmpfile.name, 2) #doctest: +NORMALIZE_WHITESPACE\n",
      "            True\n",
      "                document token  weight\n",
      "            0         0  this     0.5\n",
      "            1         0    is     0.4\n",
      "\n",
      "DATA\n",
      "    log = <Logger dariah_topics (WARNING)>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\acer\\anaconda3\\lib\\site-packages\\dariah_topics\\postprocessing.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(postprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module cophi_toolbox.preprocessing in cophi_toolbox:\n",
      "\n",
      "NAME\n",
      "    cophi_toolbox.preprocessing\n",
      "\n",
      "DESCRIPTION\n",
      "    Preprocessing Text Data, Creating Matrices and Cleaning Corpora\n",
      "    ***************************************************************\n",
      "    \n",
      "    Functions of this module are for **preprocessing purpose**. You can read text files, `tokenize <https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)>`_ and segment documents (if a document is chunked into smaller segments, each segment counts as one document), create and read `document-term matrices <https://en.wikipedia.org/wiki/Document-term_matrix>`_, determine and remove features. Recurrent variable names are based on the following conventions:\n",
      "    \n",
      "        * ``corpus`` means an iterable containing at least one ``document``.\n",
      "        * ``document`` means one single string containing all characters of a text     file, including whitespaces, punctuations, numbers, etc.\n",
      "        * ``dkpro_document`` means a pandas DataFrame containing tokens and additional     information, e.g. *part-of-speech tags* or *lemmas*, produced by `DARIAH-DKPro-Wrapper <https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper>`_.\n",
      "        * ``tokenized_corpus`` means an iterable containing at least one ``tokenized_document``     or ``dkpro_document``.\n",
      "        * ``tokenized_document`` means an iterable containing tokens of a ``document``.\n",
      "        * ``document_labels`` means an iterable containing names of each ``document``     and must have as much elements as ``corpus`` or ``tokenized_corpus`` does.\n",
      "        * ``document_term_matrix`` means either a pandas DataFrame with rows corresponding to     ``document_labels`` and columns to types (distinct tokens in the corpus), whose     values are token frequencies, or a pandas DataFrame with a MultiIndex     and only one column corresponding to word frequencies. The first column of the     MultiIndex corresponds to a document ID (based on ``document_labels``) and the     second column to a type ID. The first variant is designed for small and the     second for large corpora.\n",
      "        * ``token2id`` means a dictionary containing a token as key and an unique identifier     as key, e.g. ``{'first_document': 0, 'second_document': 1}``.\n",
      "    \n",
      "    Contents\n",
      "    ********\n",
      "        * :func:`create_document_term_matrix()` creates a document-term matrix, for either     small or large corpora.\n",
      "        * :func:`filter_pos_tags()` filters a ``dkpro_document`` by specific     *part-of-speech tags* and returns either tokens or, if available, lemmas.\n",
      "        * :func:`find_hapax_legomena()` determines *hapax legomena* based on frequencies     of a ``document_term_matrix``.\n",
      "        * :func:`list_mfw()` determines *most frequent words* based on frequencies     of a ``document_term_matrix``.\n",
      "        * :func:`read_document_term_matrix()` reads a document-term matrix from a CSV file.\n",
      "        * :func:`read_files()` reads one or multiple files based on a pathlist.\n",
      "        * :func:`remove_features()` removes features from a ``document_term_matrix``.\n",
      "        * :func:`segment()` is a wrapper for :func:`segment_fuzzy()` and segments a     ``tokenized_document`` into segments of a certain number of tokens, respecting existing chunks.\n",
      "        * :func:`segment_fuzzy()` segments a ``tokenized_document``, tolerating existing     chunks (like paragraphs).\n",
      "        * :func:`split_paragraphs()` splits a ``document`` or ``dkpro_document`` by paragraphs.\n",
      "        * :func:`tokenize()` tokenizes a ``document`` based on a Unicode regular expression.\n",
      "\n",
      "FUNCTIONS\n",
      "    create_document_term_matrix(tokenized_corpus, document_labels, large_corpus=False)\n",
      "        Creates a document-term matrix.\n",
      "        \n",
      "        With this function you can create a document-term-matrix where rows     correspond to documents in the collection and columns correspond to terms.     Use the function :func:`read_files()` to read and :func:`tokenize()`     to tokenize your text files.\n",
      "        \n",
      "        Args:\n",
      "            tokenized_corpus (list): Tokenized corpus as an iterable containing one\n",
      "                or more iterables containing tokens.\n",
      "            document_labels (list): Name or label of each text file.\n",
      "            large_corpus (bool, optional): Set to True, if ``tokenized_corpus`` is\n",
      "                very large. Defaults to False.\n",
      "        \n",
      "        Returns:\n",
      "            Document-term matrix as pandas DataFrame.\n",
      "        \n",
      "        Example:\n",
      "            >>> tokenized_corpus = [['this', 'is', 'document', 'one'], ['this', 'is', 'document', 'two']]\n",
      "            >>> document_labels = ['document_one', 'document_two']\n",
      "            >>> create_document_term_matrix(tokenized_corpus, document_labels) #doctest: +NORMALIZE_WHITESPACE\n",
      "                          this   is  document  two  one\n",
      "            document_one   1.0  1.0       1.0  0.0  1.0\n",
      "            document_two   1.0  1.0       1.0  1.0  0.0\n",
      "            >>> document_term_matrix, document_ids, type_ids = create_document_term_matrix(tokenized_corpus, document_labels, True)\n",
      "            >>> isinstance(document_term_matrix, pd.DataFrame) and isinstance(document_ids, dict) and isinstance(type_ids, dict)\n",
      "            True\n",
      "    \n",
      "    filter_pos_tags(dkpro_document, pos_tags=['ADJ', 'V', 'NN'], lemma=True)\n",
      "        Gets tokens or lemmas respectively of selected POS-tags from pandas DataFrame.\n",
      "        \n",
      "        With this function you can filter `DARIAH-DKPro-Wrapper <https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper>`_     output. Commit a list of POS-tags to get specific tokens (if ``lemma`` False)     or lemmas (if ``lemma`` True). \n",
      "        Use the function :func:`read_files()` to read CSV files.\n",
      "        \n",
      "        Args:\n",
      "            dkpro_document (pandas.DataFrame): DARIAH-DKPro-Wrapper output.\n",
      "            pos_tags (list, optional): List of desired POS-tags. Defaults\n",
      "                to ``['ADJ', 'V', 'NN']``.\n",
      "            lemma (bool, optional): If True, lemmas will be selected, otherwise tokens.\n",
      "                Defaults to True.\n",
      "        \n",
      "        Yields:\n",
      "            A pandas DataFrame containing tokens or lemmas.\n",
      "        \n",
      "        Example:\n",
      "            >>> dkpro_document = pd.DataFrame({'CPOS': ['ART', 'V', 'ART', 'NN'],\n",
      "            ...                                'Token': ['this', 'was', 'a', 'document'],\n",
      "            ...                                'Lemma': ['this', 'is', 'a', 'document']})\n",
      "            >>> list(filter_pos_tags(dkpro_document)) #doctest: +NORMALIZE_WHITESPACE\n",
      "            [1          is\n",
      "            3    document\n",
      "            Name: Lemma, dtype: object]\n",
      "    \n",
      "    find_hapax_legomena(document_term_matrix, type_ids=None)\n",
      "        Creates a list with hapax legommena.\n",
      "        \n",
      "        With this function you can determine *hapax legomena* for each document.     Use the function :func:`create_document_term_matrix()` to create a     document-term matrix.\n",
      "        \n",
      "        Args:\n",
      "            document_term_matrix (pandas.DataFrame): A document-term matrix.\n",
      "            type_ids (dict): A dictionary with types as key and identifiers as values.\n",
      "                If ``document_term_matrix`` is designed for large corpora, you have\n",
      "                to commit ``type_ids``, too.\n",
      "        \n",
      "        Returns:\n",
      "            Hapax legomena in a list.\n",
      "        \n",
      "        Example:\n",
      "            >>> document_labels = ['document']\n",
      "            >>> tokenized_corpus = [['hapax', 'stopword', 'stopword']]\n",
      "            >>> document_term_matrix = create_document_term_matrix(tokenized_corpus, document_labels)\n",
      "            >>> find_hapax_legomena(document_term_matrix)\n",
      "            ['hapax']\n",
      "            >>> document_term_matrix, _, type_ids = create_document_term_matrix(tokenized_corpus, document_labels, large_corpus=True)\n",
      "            >>> find_hapax_legomena(document_term_matrix, type_ids)\n",
      "            ['hapax']\n",
      "    \n",
      "    list_mfw(document_term_matrix, most_frequent_tokens=100, type_ids=None)\n",
      "        Creates a list with stopword based on most frequent tokens.\n",
      "        \n",
      "        With this function you can determine *most frequent tokens*, also known as     *stopwords*. First, you have to translate your corpus into a document-term     matrix.\n",
      "        Use the function :func:`create_document_term_matrix()` to create a     document-term matrix.\n",
      "        \n",
      "        Args:\n",
      "            document_term_matrix (pandas.DataFrame): A document-term matrix.\n",
      "            most_frequent_tokens (int, optional): Treshold for most frequent tokens.\n",
      "            type_ids (dict): If ``document_term_matrix`` is designed for large corpora,\n",
      "                you have to commit ``type_ids``, too.\n",
      "        \n",
      "        Returns:\n",
      "            Most frequent tokens in a list.\n",
      "        \n",
      "        Example:\n",
      "            >>> document_labels = ['document']\n",
      "            >>> tokenized_corpus = [['hapax', 'stopword', 'stopword']]\n",
      "            >>> document_term_matrix = create_document_term_matrix(tokenized_corpus, document_labels)\n",
      "            >>> list_mfw(document_term_matrix, 1)\n",
      "            ['stopword']\n",
      "            >>> document_term_matrix, _, type_ids = create_document_term_matrix(tokenized_corpus, document_labels, large_corpus=True)\n",
      "            >>> list_mfw(document_term_matrix, 1, type_ids)\n",
      "            ['stopword']\n",
      "    \n",
      "    read_document_term_matrix(filepath)\n",
      "        Reads a document-term matrix from CSV file.\n",
      "        \n",
      "        With this function you can read a CSV file containing a document-term     matrix.\n",
      "        Use the function :func:`create_document_term_matrix()` to create a document-term     matrix.\n",
      "        \n",
      "        Args:\n",
      "            filepath (str): Path to CSV file.\n",
      "        \n",
      "        Returns:\n",
      "            A document-term matrix as pandas DataFrame.\n",
      "        \n",
      "        Example:\n",
      "            >>> import tempfile\n",
      "            >>> with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as tmpfile:\n",
      "            ...     tmpfile.write(b',this,is,an,example,text\\ndocument,1,0,1,0,1') and True\n",
      "            ...     tmpfile.close()\n",
      "            ...     read_document_term_matrix(tmpfile.name) #doctest: +NORMALIZE_WHITESPACE\n",
      "            True\n",
      "                      this  is  an  example  text\n",
      "            document     1   0   1        0     1\n",
      "            >>> with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as tmpfile:\n",
      "            ...     tmpfile.write(b'document_id,type_id,0\\n1,1,1') and True\n",
      "            ...     tmpfile.close()\n",
      "            ...     read_document_term_matrix(tmpfile.name) #doctest: +NORMALIZE_WHITESPACE\n",
      "            True\n",
      "                                 0\n",
      "            document_id type_id   \n",
      "            1           1        1\n",
      "    \n",
      "    read_files(pathlist, file_format=None, xpath_expression='//tei:text', sep='\\t', csv_columns=None)\n",
      "        Reads text files based on a pathlist.\n",
      "        \n",
      "        With this function you can read multiple file formats:\n",
      "            * Plain text files (``.txt``).\n",
      "            * TEI XML files (``.xml``).\n",
      "            * CSV files (``.csv``), e.g. produced by `DARIAH-DKPro-Wrapper <https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper>`_. \n",
      "        \n",
      "        The argument ``pathlist`` is an iterable of full or relative paths. In case of     CSV files, you have the ability to select specific columns via ``columns``.     If there are multiple file formats in ``pathlist``, do not specify ``file_format``     and file extensions will be considered.\n",
      "        \n",
      "        Args:\n",
      "            pathlist (list): One or more paths to text files.\n",
      "            file_format (str, optional): Format of the files. Possible values are\n",
      "                ``text``, ``xml`` and ``csv`. If None, file extensions will be considered.\n",
      "                Defaults to None.\n",
      "            xpath_expression (str, optional): XPath expressions to match part of the\n",
      "                XML file. Defaults to ``//tei:text``.\n",
      "            sep (str, optional): Separator of CSV file. Defaults to ``'\\t'``\n",
      "            columns (list, optional): Column name or names for CSV files. If None, the\n",
      "                whole file will be processed. Defaults to None.\n",
      "        \n",
      "        Yields:\n",
      "            A ``document`` as str or, in case of a CSV file, a ``dkpro_document`` as a pandas DataFrame.\n",
      "        \n",
      "        Raises:\n",
      "            ValueError, if ``file_format`` is not supported.\n",
      "        \n",
      "        Example:\n",
      "            >>> import tempfile\n",
      "            >>> with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as first:\n",
      "            ...     pathlist = []\n",
      "            ...     first.write(b\"This is the first example.\") and True\n",
      "            ...     first.flush()\n",
      "            ...     pathlist.append(first.name)\n",
      "            ...     with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as second:\n",
      "            ...         second.write(b\"This is the second example.\") and True\n",
      "            ...         second.close()\n",
      "            ...         pathlist.append(second.name)\n",
      "            ...         list(read_files(pathlist, 'text'))\n",
      "            True\n",
      "            True\n",
      "            ['This is the first example.', 'This is the second example.']\n",
      "    \n",
      "    remove_features(features, document_term_matrix=None, tokenized_corpus=None, type_ids=None)\n",
      "        Removes features based on a list of tokens.\n",
      "        \n",
      "        With this function you can clean your corpus (either a document-term matrix     or a ``tokenized_corpus``) from *stopwords* and *hapax legomena*.\n",
      "        Use the function :func:`create_document_term_matrix()` or :func:`tokenize` to     create a document-term matrix or to tokenize your corpus, respectively.\n",
      "        \n",
      "        Args:\n",
      "            features (list): A list of tokens.\n",
      "            document_term_matrix (pandas.DataFrame, optional): A document-term matrix.\n",
      "            tokenized_corpus (list, optional): An iterable of one or more ``tokenized_document``.\n",
      "            type_ids (dict, optional): A dictionary with types as key and identifiers as values.\n",
      "        \n",
      "        Returns:\n",
      "            A clean document-term matrix as pandas DataFrame or ``tokenized_corpus`` as list.\n",
      "        \n",
      "        Example:\n",
      "            >>> document_labels = ['document']\n",
      "            >>> tokenized_corpus = [['this', 'is', 'a', 'document']]\n",
      "            >>> document_term_matrix = create_document_term_matrix(tokenized_corpus, document_labels)\n",
      "            >>> features = ['this']\n",
      "            >>> remove_features(features, document_term_matrix) #doctest: +NORMALIZE_WHITESPACE\n",
      "                       is  document    a\n",
      "            document  1.0       1.0  1.0\n",
      "            >>> document_term_matrix, _, type_ids = create_document_term_matrix(tokenized_corpus, document_labels, large_corpus=True)\n",
      "            >>> len(remove_features(features, document_term_matrix, type_ids=type_ids))\n",
      "            3\n",
      "            >>> list(remove_features(features, tokenized_corpus=tokenized_corpus))\n",
      "            [['is', 'a', 'document']]\n",
      "    \n",
      "    segment(document, segment_size=1000, tolerance=0, chunker=None, tokenizer=None, flatten_chunks=True, materialize=True)\n",
      "        Segments a document into segments of about ``segment_size`` tokens, respecting existing chunks.\n",
      "        \n",
      "        Consider you have a document. You wish to split the document into     segments of about 1000 tokens, but you prefer to keep paragraphs together     if this does not increase or decrease the token size by more than 5%.\n",
      "        This is a convenience wrapper around :func:`segment_fuzzy()`.\n",
      "        \n",
      "        Args:\n",
      "            document (list): The document to process. This is an iterable of\n",
      "                chunks, each of which is an iterable of tokens.\n",
      "            segment_size (int): The target size of each segment, in tokens. Defaults\n",
      "                to 1000.\n",
      "            tolerance (float, optional): How much may the actual segment size differ from\n",
      "                the segment_size? If ``0 < tolerance < 1``, this is interpreted as a\n",
      "                fraction of the segment_size, otherwise it is interpreted as an\n",
      "                absolute number. If ``tolerance < 0``, chunks are never split apart.\n",
      "                Defaults to None.\n",
      "            chunker (callable, optional): A one-argument function that cuts the document into\n",
      "                chunks. If this is present, it is called on the given document.\n",
      "                Defaults to None.\n",
      "            tokenizer (callable, optional): A one-argument function that tokenizes each chunk.\n",
      "                Defaults to None.\n",
      "            flatten_chunks (bool, optional): If True, undo the effect of the chunker by\n",
      "                chaining the chunks in each segment, thus each segment consists of\n",
      "                tokens. This can also be a one-argument function in order to\n",
      "                customize the un-chunking. Defaults to True.\n",
      "            materialize (bool, optional): If True, materializes the segments. Defaults to True.\n",
      "        \n",
      "        Example:\n",
      "            >>> segment([['This', 'is', 'the', 'first', 'chunk'],\n",
      "            ...          ['this', 'is', 'the', 'second', 'chunk']], 2) #doctest: +NORMALIZE_WHITESPACE\n",
      "            [['This', 'is'],\n",
      "            ['the', 'first'],\n",
      "            ['chunk', 'this'],\n",
      "            ['is', 'the'],\n",
      "            ['second', 'chunk']]\n",
      "    \n",
      "    segment_fuzzy(document, segment_size=5000, tolerance=0.05)\n",
      "        Segments a document, tolerating existing chunks (like paragraphs).\n",
      "        \n",
      "        Consider you have a ``document``. You wish to split the ``document`` into     segments of about 1000 tokens, but you prefer to keep paragraphs together     if this does not increase or decrease the token size by more than 5%.\n",
      "        \n",
      "        Args:\n",
      "            document (list): The document to process. This is an iterable of\n",
      "                chunks, each of which is an iterable of tokens.\n",
      "            segment_size (int, optional): The target length of each segment in tokens.\n",
      "                Defaults to 5000.\n",
      "            tolerance (float, optional): How much may the actual segment size differ from\n",
      "                the ``segment_size``? If ``0 < tolerance < 1``, this is interpreted as a\n",
      "                fraction of the ``segment_size``, otherwise it is interpreted as an\n",
      "                absolute number. If ``tolerance < 0``, chunks are never split apart.\n",
      "                Defaults to 0.05.\n",
      "        \n",
      "        Yields:\n",
      "            Segments. Each segment is a list of chunks, each chunk is a list of\n",
      "            tokens.\n",
      "        \n",
      "        Example:\n",
      "            >>> list(segment_fuzzy([['This', 'is', 'the', 'first', 'chunk'],\n",
      "            ...                     ['this', 'is', 'the', 'second', 'chunk']], 2)) #doctest: +NORMALIZE_WHITESPACE\n",
      "            [[['This', 'is']],\n",
      "            [['the', 'first']],\n",
      "            [['chunk'], ['this']],\n",
      "            [['is', 'the']],\n",
      "            [['second', 'chunk']]]\n",
      "    \n",
      "    split_paragraphs(document, sep=regex.Regex('\\\\n', flags=regex.V0))\n",
      "        Splits the given document by paragraphs.\n",
      "        \n",
      "        With this function you can split a document by paragraphs. In case of a     document as str, you also have the ability to select a certain regular     expression to split the document.\n",
      "        Use the function :func:`read_files()` to read files.\n",
      "        \n",
      "        Args:\n",
      "            document Union(str, pandas.DataFrame): Document text or DARIAH-DKPro-Wrapper output.\n",
      "            sep (regex.Regex, optional): Separator indicating a paragraph.\n",
      "        \n",
      "        Returns:\n",
      "            A list of paragraphs.\n",
      "        \n",
      "        Example:\n",
      "            >>> document = \"First paragraph\\nsecond paragraph.\"\n",
      "            >>> split_paragraphs(document)\n",
      "            ['First paragraph', 'second paragraph.']\n",
      "            >>> dkpro_document = pd.DataFrame({'Token': ['first', 'paragraph', 'second', 'paragraph', '.'],\n",
      "            ...                                'ParagraphId': [1, 1, 2, 2, 2]})\n",
      "            >>> split_paragraphs(dkpro_document)[0] #doctest: +NORMALIZE_WHITESPACE\n",
      "                             Token\n",
      "            ParagraphId           \n",
      "            1                first\n",
      "            1            paragraph\n",
      "    \n",
      "    tokenize(document, pattern='\\\\p{L}+\\\\p{P}?\\\\p{L}+', lower=True)\n",
      "        Tokenizes with Unicode regular expressions.\n",
      "        \n",
      "        With this function you can tokenize a ``document`` with a regular expression.     You also have the ability to commit your own regular expression. The default     expression is ``\\p{Letter}+\\p{Punctuation}?\\p{Letter}+``, which means one or     more letters, followed by one or no punctuation, followed by one or more     letters. So, one letter words will not match. In case you want to lower     all tokens, set the argument ``lower`` to True (it is by default).    \n",
      "        Use the functions :func:`read_files()` to read your text files.\n",
      "        \n",
      "        Args:\n",
      "            document (str): Document text.\n",
      "            pattern (str, optional): Regular expression to match tokens.\n",
      "            lower (boolean, optional): If True, lowers all characters. Defaults to True.\n",
      "        \n",
      "        Yields:\n",
      "            All matching tokens in the ``document``.\n",
      "        \n",
      "        Example:\n",
      "            >>> list(tokenize(\"This is 1 example text.\"))\n",
      "            ['this', 'is', 'example', 'text']\n",
      "\n",
      "DATA\n",
      "    log = <Logger cophi_toolbox.preprocessing (WARNING)>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\acer\\anaconda3\\lib\\site-packages\\cophi_toolbox\\preprocessing.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will need some additional functions from external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metadata_toolbox.utils as metadata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not pay heed to any warnings right now and execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Reading a corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the path to the corpus folder\n",
    "\n",
    "In the present example code, we are using the 30 diary excerpts from the folder `grenzboten`. To use your own corpus, change the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = Path('data', 'grenzboten_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the pattern of filenames for metadata extraction\n",
    "\n",
    "You have the ability to extract metadata from the filenames. For instance, if your textfiles look like:\n",
    "\n",
    "```\n",
    "goethe_1816_stella.txt\n",
    "```\n",
    "\n",
    "the pattern would look like this:\n",
    "\n",
    "```\n",
    "{author}_{year}_{title}\n",
    "```\n",
    "\n",
    "So, let's try this for the example corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '{author}_{year}_{title}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing file paths and metadata\n",
    "We begin by creating a list of all the documents in the folder specified above. That list will tell the function `preprocessing.read_files` (see below) which text documents to read. Furthermore, based on filenames we can create some metadata, e.g. author and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Beck_1844_Tagebuch_56.txt</th>\n",
       "      <td>Beck</td>\n",
       "      <td>1844</td>\n",
       "      <td>Tagebuch_56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Berto_1915_Kriegstagebuch_94.txt</th>\n",
       "      <td>Berto</td>\n",
       "      <td>1915</td>\n",
       "      <td>Kriegstagebuch_94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Castelli_1846_Tagebuch_51.txt</th>\n",
       "      <td>Castelli</td>\n",
       "      <td>1846</td>\n",
       "      <td>Tagebuch_51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Cleinom_1914_Kriegstagebuch_94.txt</th>\n",
       "      <td>Cleinom</td>\n",
       "      <td>1914</td>\n",
       "      <td>Kriegstagebuch_94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Dix_1914_Kriegstagebuch_37.txt</th>\n",
       "      <td>Dix</td>\n",
       "      <td>1914</td>\n",
       "      <td>Kriegstagebuch_37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      author  year  \\\n",
       "data\\grenzboten_sample\\Beck_1844_Tagebuch_56.txt        Beck  1844   \n",
       "data\\grenzboten_sample\\Berto_1915_Kriegstagebuc...     Berto  1915   \n",
       "data\\grenzboten_sample\\Castelli_1846_Tagebuch_5...  Castelli  1846   \n",
       "data\\grenzboten_sample\\Cleinom_1914_Kriegstageb...   Cleinom  1914   \n",
       "data\\grenzboten_sample\\Dix_1914_Kriegstagebuch_...       Dix  1914   \n",
       "\n",
       "                                                                title  \n",
       "data\\grenzboten_sample\\Beck_1844_Tagebuch_56.txt          Tagebuch_56  \n",
       "data\\grenzboten_sample\\Berto_1915_Kriegstagebuc...  Kriegstagebuch_94  \n",
       "data\\grenzboten_sample\\Castelli_1846_Tagebuch_5...        Tagebuch_51  \n",
       "data\\grenzboten_sample\\Cleinom_1914_Kriegstageb...  Kriegstagebuch_94  \n",
       "data\\grenzboten_sample\\Dix_1914_Kriegstagebuch_...  Kriegstagebuch_37  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.concat([metadata.fname2metadata(str(path), pattern=pattern) for path in path_to_corpus.glob('*.txt')])\n",
    "meta[:5] # by adding '[:5]' to the variable, only the first 5 elements will be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read listed documents from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tagebuch von Karl Beck. Man spricht seit vierzehn Tagen von einem vollstÃ¤ndigen Ministerwechsel und es circuliren im Publicum die verschiedensten Combinationen, wobei heute ganz andere Namen genannt werden, als gestern und morgen wieder andere, als heute.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(preprocessing.read_files(meta.index))\n",
    "corpus[0][:255] # printing the first 255 characters of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `corpus` contains as much elements (`documents`) as texts in your corpus are. Each element of `corpus` is a list containing exactly one element, the text itself as one single string including all whitespaces and punctuations:\n",
    "\n",
    "```\n",
    "[['This is the content of your first document.'],\n",
    " ['This is the content of your second document.'],\n",
    " ...\n",
    " ['This is the content of your last document.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tokenize corpus\n",
    "Now, your `documents` in `corpus` will be tokenized. Tokenization is the task of cutting a stream of characters into linguistic units, simply words or, more precisely, tokens. The tokenize function `dariah_topics` provides is a simple Unicode tokenizer. Depending on the corpus, it might be useful to use an external tokenizer function, or even develop your own, since its efficiency varies with language, epoch and text type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [list(preprocessing.tokenize(document)) for document in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, each `document` is represented by a list of separate token strings. As above, have a look at the first document (which has the index `0` as Python starts counting at 0) and show its first 14 words/tokens (that have the indices `0:13` accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagebuch',\n",
       " 'von',\n",
       " 'karl',\n",
       " 'beck',\n",
       " 'man',\n",
       " 'spricht',\n",
       " 'seit',\n",
       " 'vierzehn',\n",
       " 'tagen',\n",
       " 'von',\n",
       " 'einem',\n",
       " 'vollstÃ¤ndigen',\n",
       " 'ministerwechsel']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[0][0:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create a document-term matrix\n",
    "\n",
    "The LDA topic model is based on a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) of the corpus. In a document-term matrix, rows correspond to documents and columns correspond to terms or tokens respectively. The values are token frequencies for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>die</th>\n",
       "      <th>der</th>\n",
       "      <th>und</th>\n",
       "      <th>in</th>\n",
       "      <th>den</th>\n",
       "      <th>von</th>\n",
       "      <th>zu</th>\n",
       "      <th>das</th>\n",
       "      <th>des</th>\n",
       "      <th>nicht</th>\n",
       "      <th>...</th>\n",
       "      <th>weitlinge</th>\n",
       "      <th>weitschichtige</th>\n",
       "      <th>welker</th>\n",
       "      <th>welscher</th>\n",
       "      <th>werthschÃ¤tzung</th>\n",
       "      <th>wesentlicher</th>\n",
       "      <th>wichtigeren</th>\n",
       "      <th>widerliche</th>\n",
       "      <th>widersetzlichen</th>\n",
       "      <th>gasfrage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tagebuch_56</th>\n",
       "      <td>90.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_94</th>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tagebuch_51</th>\n",
       "      <td>226.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_94</th>\n",
       "      <td>39.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_37</th>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     die    der    und     in   den   von    zu   das   des  \\\n",
       "Tagebuch_56         90.0   92.0   84.0   70.0  30.0  26.0  25.0  16.0  25.0   \n",
       "Kriegstagebuch_94   11.0   32.0   24.0   12.0   8.0  17.0   0.0   3.0   5.0   \n",
       "Tagebuch_51        226.0  177.0  188.0  111.0  73.0  62.0  93.0  60.0  35.0   \n",
       "Kriegstagebuch_94   39.0   48.0   34.0   28.0  15.0  25.0   4.0   5.0  11.0   \n",
       "Kriegstagebuch_37   40.0   34.0   15.0   17.0  10.0  19.0   5.0   6.0  18.0   \n",
       "\n",
       "                   nicht  ...  weitlinge  weitschichtige  welker  welscher  \\\n",
       "Tagebuch_56         23.0  ...        0.0             0.0     0.0       0.0   \n",
       "Kriegstagebuch_94    1.0  ...        0.0             0.0     0.0       0.0   \n",
       "Tagebuch_51         78.0  ...        0.0             0.0     0.0       0.0   \n",
       "Kriegstagebuch_94    3.0  ...        0.0             0.0     0.0       0.0   \n",
       "Kriegstagebuch_37    3.0  ...        0.0             0.0     0.0       0.0   \n",
       "\n",
       "                   werthschÃ¤tzung  wesentlicher  wichtigeren  widerliche  \\\n",
       "Tagebuch_56                   0.0           0.0          0.0         0.0   \n",
       "Kriegstagebuch_94             0.0           0.0          0.0         0.0   \n",
       "Tagebuch_51                   0.0           0.0          0.0         0.0   \n",
       "Kriegstagebuch_94             0.0           0.0          0.0         0.0   \n",
       "Kriegstagebuch_37             0.0           0.0          0.0         0.0   \n",
       "\n",
       "                   widersetzlichen  gasfrage  \n",
       "Tagebuch_56                    0.0       0.0  \n",
       "Kriegstagebuch_94              0.0       0.0  \n",
       "Tagebuch_51                    0.0       0.0  \n",
       "Kriegstagebuch_94              0.0       0.0  \n",
       "Kriegstagebuch_37              0.0       0.0  \n",
       "\n",
       "[5 rows x 24451 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix = preprocessing.create_document_term_matrix(tokenized_corpus, meta['title'])\n",
    "document_term_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Feature removal\n",
    "\n",
    "*Stopwords* (also known as *most frequent tokens*) and *hapax legomena* are harmful for LDA and have to be removed from the corpus or the document-term matrix respectively. In this example, the 50 most frequent tokens will be categorized as stopwords.\n",
    "\n",
    "**Hint**: Be careful with removing most frequent tokens, you might remove tokens quite important for LDA. Anyway, to gain better results, it is highly recommended to use an external stopwords list.\n",
    "\n",
    "In this notebook, we combine the 50 most frequent tokens, hapax legomena and an external stopwordslist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the 100 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = preprocessing.list_mfw(document_term_matrix, most_frequent_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the five most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die', 'der', 'und', 'in', 'den']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List hapax legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of types in corpus: 24451\n",
      "Total number of hapax legomena: 19757\n"
     ]
    }
   ],
   "source": [
    "hapax_legomena = preprocessing.find_hapax_legomena(document_term_matrix)\n",
    "print(\"Total number of types in corpus:\", document_term_matrix.shape[1])\n",
    "print(\"Total number of hapax legomena:\", len(hapax_legomena))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function find_hapax_legomena in module cophi_toolbox.preprocessing:\n",
      "\n",
      "find_hapax_legomena(document_term_matrix, type_ids=None)\n",
      "    Creates a list with hapax legommena.\n",
      "    \n",
      "    With this function you can determine *hapax legomena* for each document.     Use the function :func:`create_document_term_matrix()` to create a     document-term matrix.\n",
      "    \n",
      "    Args:\n",
      "        document_term_matrix (pandas.DataFrame): A document-term matrix.\n",
      "        type_ids (dict): A dictionary with types as key and identifiers as values.\n",
      "            If ``document_term_matrix`` is designed for large corpora, you have\n",
      "            to commit ``type_ids``, too.\n",
      "    \n",
      "    Returns:\n",
      "        Hapax legomena in a list.\n",
      "    \n",
      "    Example:\n",
      "        >>> document_labels = ['document']\n",
      "        >>> tokenized_corpus = [['hapax', 'stopword', 'stopword']]\n",
      "        >>> document_term_matrix = create_document_term_matrix(tokenized_corpus, document_labels)\n",
      "        >>> find_hapax_legomena(document_term_matrix)\n",
      "        ['hapax']\n",
      "        >>> document_term_matrix, _, type_ids = create_document_term_matrix(tokenized_corpus, document_labels, large_corpus=True)\n",
      "        >>> find_hapax_legomena(document_term_matrix, type_ids)\n",
      "        ['hapax']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessing.find_hapax_legomena)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Use external stopwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stopwordlist = Path('data', 'stopwords', 'de.txt')\n",
    "external_stopwords = [line.strip() for line in path_to_stopwordlist.open('r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine lists and remove content from `document_term_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = stopwords + hapax_legomena + external_stopwords\n",
    "document_term_matrix = preprocessing.remove_features(features, document_term_matrix=document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is how your clean corpus looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>franzosen</th>\n",
       "      <th>genommen</th>\n",
       "      <th>abgewiesen</th>\n",
       "      <th>sÃ¼dlich</th>\n",
       "      <th>berlin</th>\n",
       "      <th>lassen</th>\n",
       "      <th>geschÃ¼tze</th>\n",
       "      <th>englische</th>\n",
       "      <th>deutschland</th>\n",
       "      <th>januar</th>\n",
       "      <th>...</th>\n",
       "      <th>schlechteste</th>\n",
       "      <th>dubatowka</th>\n",
       "      <th>palameix</th>\n",
       "      <th>verschlossene</th>\n",
       "      <th>eimer</th>\n",
       "      <th>schicksale</th>\n",
       "      <th>eilwagen</th>\n",
       "      <th>klippe</th>\n",
       "      <th>rennt</th>\n",
       "      <th>zuschrieb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tagebuch_56</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_94</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tagebuch_51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_94</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_37</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   franzosen  genommen  abgewiesen  sÃ¼dlich  berlin  lassen  \\\n",
       "Tagebuch_56              0.0       1.0         0.0      0.0     4.0     3.0   \n",
       "Kriegstagebuch_94        0.0       9.0         3.0      5.0     3.0     0.0   \n",
       "Tagebuch_51              1.0       1.0         1.0      0.0     1.0     7.0   \n",
       "Kriegstagebuch_94        4.0       3.0         2.0      3.0     2.0     0.0   \n",
       "Kriegstagebuch_37        6.0       1.0         0.0      0.0     0.0     0.0   \n",
       "\n",
       "                   geschÃ¼tze  englische  deutschland  januar  ...  \\\n",
       "Tagebuch_56              0.0        0.0          4.0     0.0  ...   \n",
       "Kriegstagebuch_94        6.0        3.0          0.0     0.0  ...   \n",
       "Tagebuch_51              0.0        1.0          9.0     0.0  ...   \n",
       "Kriegstagebuch_94        2.0        7.0          1.0     0.0  ...   \n",
       "Kriegstagebuch_37        5.0        1.0          1.0     0.0  ...   \n",
       "\n",
       "                   schlechteste  dubatowka  palameix  verschlossene  eimer  \\\n",
       "Tagebuch_56                 0.0        0.0       0.0            0.0    0.0   \n",
       "Kriegstagebuch_94           0.0        0.0       0.0            0.0    0.0   \n",
       "Tagebuch_51                 0.0        0.0       0.0            2.0    0.0   \n",
       "Kriegstagebuch_94           0.0        0.0       0.0            0.0    0.0   \n",
       "Kriegstagebuch_37           0.0        0.0       0.0            0.0    0.0   \n",
       "\n",
       "                   schicksale  eilwagen  klippe  rennt  zuschrieb  \n",
       "Tagebuch_56               0.0       0.0     0.0    0.0        0.0  \n",
       "Kriegstagebuch_94         0.0       0.0     0.0    0.0        0.0  \n",
       "Tagebuch_51               0.0       0.0     0.0    0.0        0.0  \n",
       "Kriegstagebuch_94         0.0       0.0     0.0    0.0        0.0  \n",
       "Kriegstagebuch_37         0.0       0.0     0.0    0.0        0.0  \n",
       "\n",
       "[5 rows x 4242 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model creation\n",
    "\n",
    "The actual topic modeling is done with external state-of-the-art LDA implementations. In this example, we are relying on the open-source toolkit **lda**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creating list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate numbers back into words after the model creation, you have to set up a list of all unique tokens in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['franzosen', 'genommen', 'abgewiesen', 'sÃ¼dlich', 'berlin', 'lassen',\n",
       "       'geschÃ¼tze', 'englische', 'deutschland', 'januar',\n",
       "       ...\n",
       "       'schlechteste', 'dubatowka', 'palameix', 'verschlossene', 'eimer',\n",
       "       'schicksale', 'eilwagen', 'klippe', 'rennt', 'zuschrieb'],\n",
       "      dtype='object', length=4242)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = document_term_matrix.columns\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Translate document-term matrix into an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all values of your document-term matrix will be translated into an [array](https://en.wikipedia.org/wiki/Array_data_structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0,  9,  3, ...,  0,  0,  0],\n",
       "       [ 1,  1,  1, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 6,  6, 19, ...,  0,  0,  0],\n",
       "       [ 3,  2,  0, ...,  0,  0,  0],\n",
       "       [ 1,  3,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix_arr = document_term_matrix.values.astype(int)\n",
    "document_term_matrix_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Generate LDA model\n",
    "\n",
    "We use the class `LDA` from the library `lda` (which is basically not the same, because Python is case sensitive) to generate a LDA topic model. To instance a `LDA` object, there have to be specified a couple of parameters.\n",
    "\n",
    "But first, if you are curious about any library, module, class or function, try `help()`. This can be very useful, because (at least in a well documented library) explanations of use and parameters will be printed. We're interested in the class `LDA` of the library `lda`, so let's try:\n",
    "\n",
    "```\n",
    "help(lda.LDA)\n",
    "```\n",
    "\n",
    "This will print something like this (in fact even more):\n",
    "\n",
    "```\n",
    "Help on class LDA in module lda.lda:\n",
    "\n",
    "class LDA(builtins.object)\n",
    " |  Latent Dirichlet allocation using collapsed Gibbs sampling\n",
    " |  \n",
    " |  Parameters\n",
    " |  ----------\n",
    " |  n_topics : int\n",
    " |      Number of topics\n",
    " |  \n",
    " |  n_iter : int, default 2000\n",
    " |      Number of sampling iterations\n",
    " |  \n",
    " |  alpha : float, default 0.1\n",
    " |      Dirichlet parameter for distribution over topics\n",
    " |  \n",
    " |  eta : float, default 0.01\n",
    " |      Dirichlet parameter for distribution over words\n",
    " |  \n",
    " |  random_state : int or RandomState, optional\n",
    " |      The generator used for the initial topics.\n",
    "```\n",
    "\n",
    "So, now you know how to define the number of topics and the number of sampling iterations as well. A higher number of iterations will probably yield a better model, but also increases processing time. `alpha`, `eta` and `random_state` are so-called *hyperparameters*. They influence the model's performance, so feel free to play around with them. In the present example, we will leave the default values. Furthermore, there exist various methods for hyperparameter optimization, e.g. gridsearch or Gaussian optimization.\n",
    "\n",
    "**Warning: This step can take quite a while!** Meaning something between some seconds and some hours depending on corpus size and the number of iterations. Our example corpus should be done within a minute or two at `n_iter=1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=1000)\n",
    "model.fit(document_term_matrix_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create document-topic matrix\n",
    "\n",
    "The generated model object can now be translated into a human-readable document-topic matrix (that is a actually a pandas DataFrame) that constitutes our principle exchange format for topic modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module lda.lda:\n",
      "\n",
      "fit(self, X, y=None)\n",
      "    Fit the model with X.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X: array-like, shape (n_samples, n_features)\n",
      "        Training data, where n_samples in the number of samples\n",
      "        and n_features is the number of features. Sparse matrix allowed.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "        Returns the instance itself.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda.LDA.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-8ee853f4fe7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "topics = postprocessing.show_topics(model=model, vocabulary=vocabulary)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function show_topics in module dariah_topics.postprocessing:\n",
      "\n",
      "show_topics(model=None, vocabulary=None, topic_keys_file=None, num_keys=10)\n",
      "    Shows topics of LDA model.\n",
      "    \n",
      "    With this function you can show all topics of a LDA model in a pandas DataFrame.     For each topic, the top ``num_keys`` keys will be considered. If you have a\n",
      "    * `lda <https://pypi.python.org/pypi/lda>`_ model, you have to pass the model     as ``model`` and the document-term matrix vocabulary as ``vocabulary``.\n",
      "    * `Gensim <https://radimrehurek.com/gensim/>`_ model, you have to pass only the model     as ``model``.\n",
      "    * `MALLET <http://mallet.cs.umass.edu/topics.php>`_ based workflow, you have to    pass only the ``topic_keys_file``.\n",
      "    \n",
      "    Args:\n",
      "        model (optional): lda or Gensim model.\n",
      "        vocabulary (list, optional): Only for lda. The vocabulary of the \n",
      "            document-term matrix.\n",
      "        topic_keys_file (str): Only for MALLET. Path to the topic keys file.\n",
      "        num_keys (int, optional): Number of top keys for each topic. \n",
      "    \n",
      "    Returns:\n",
      "        A pandas DataFrame with rows corresponding to topics and columns corresponding\n",
      "            to keys.\n",
      "    \n",
      "    Example:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(postprocessing.show_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each topic has a certain probability for each document in the corpus (have a look at the cell below). This probability distributions are visualized in an interactive **heatmap** (the darker the color, the higher the probability) which displays the kind of information\n",
    "                that is presumably most useful to literary scholars. Going beyond pure exploration, this visualization can be used to show thematic developments over a set of texts as well as a single text, akin to a dynamic topic model. What might become\n",
    "                apparent here, is that some topics correlate highly with a specific author or group of authors, while other topics correlate highly with a specific text or group of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a9321f3bbf65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocument_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdocument_topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "document_topics = postprocessing.show_document_topics(model=model, topics=topics, document_labels=meta['title'])\n",
    "document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics over all documents\n",
    "\n",
    "The distribution of topics over all documents can now be visualized in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3c545f10dd80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPlotDocumentTopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPlotDocumentTopics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPlotDocumentTopics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteractive_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook_handle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document_topics' is not defined"
     ]
    }
   ],
   "source": [
    "PlotDocumentTopics = visualization.PlotDocumentTopics(document_topics)\n",
    "show(PlotDocumentTopics.interactive_heatmap(), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a static heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_heatmap = PlotDocumentTopics.static_heatmap()\n",
    "static_heatmap.show()"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
