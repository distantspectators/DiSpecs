{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics â€“ Easy Topic Modeling in Python\n",
    "\n",
    "The text mining technique **Topic Modeling** has become a popular statistical method for clustering documents. This [Jupyter notebook](http://jupyter.org/) introduces a step-by-step workflow, basically containing data preprocessing, the actual topic modeling using **latent Dirichlet allocation** (LDA), which learns the relationships between words, topics, and documents, as well as some interactive visualizations to explore the model.\n",
    "\n",
    "LDA, introduced in the context of text analysis in [2003](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), is an instance of a more general class of models called **mixed-membership models**. Involving a number of distributions and parameters, the topic model is typically performed using [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) with conjugate priors and is purely based on word frequencies.\n",
    "\n",
    "There have been written numerous introductions to topic modeling for humanists (e.g. [this one](http://scottbot.net/topic-modeling-for-humanists-a-guided-tour/)), which provide another level of detail regarding its technical and epistemic properties.\n",
    "\n",
    "For this workflow, you will need a corpus (a set of texts) as plain text (`.txt`) or [TEI XML](http://www.tei-c.org/index.xml) (`.xml`). Using the `dariah_topics` package, you also have the ability to process the output of [DARIAH-DKPro-Wrapper](https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper), a command-line tool for *natural language processing*.\n",
    "\n",
    "Topic modeling works best with very large corpora. The [TextGrid Repository](https://textgridrep.org/) is a great place to start searching for text data. Anyway, to demonstrate the technique, we provide one small text collection in the folder `grenzboten_sample` containing 15 diary excerpts, as well as 15 war diary excerpts, which appeared in *Die Grenzboten*, a German newspaper of the late 19th and early 20th century.\n",
    "\n",
    "**Of course, you can work with your own corpus in this notebook.**\n",
    "\n",
    "We're relying on the LDA implementation by [Andrew McCallum](https://people.cs.umass.edu/~mccallum/), called [MALLET](http://mallet.cs.umass.edu/topics.php), which is known to be very robust. Aside from that, we provide two more Jupyter notebooks:\n",
    "\n",
    "* [IntroducingGensim](IntroducingGensim.ipynb), using LDA by [Gensim](https://radimrehurek.com/project/gensim/), which is attractive because of its multi-core support.\n",
    "* [IntroducingLda](IntroducingLda.ipynb), using LDA by [lda](http://pythonhosted.org/lda/index.html), which is very lightweight.\n",
    "\n",
    "For more information in general, have a look at the [documentation](http://dev.digital-humanities.de/ci/job/DARIAH-Topics/doclinks/1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: Installing dependencies\n",
    "\n",
    "To work within this Jupyter notebook, you will have to import the `dariah_topics` library. As you do, `dariah_topics` also imports a couple of external libraries, which have to be installed first. `pip` is the preferred installer program in Python. Starting with Python 3.4, it is included by default with the Python binary installers. If you are interested in `pip`, have a look at [this website](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "To install the `dariah_topics` library with all dependencies, open your commandline, go with `cd` to the folder `Topics` and run:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Alternatively, you can do:\n",
    "\n",
    "```\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "If you get any errors or are not able to install *all* dependencies properly, try [Stack Overflow](https://stackoverflow.com/questions/tagged/pip) for troubleshooting or create a new issue on our [GitHub page](https://github.com/DARIAH-DE/Topics).\n",
    "\n",
    "**Important**: If you are on macOS or Linux, you will have to use `pip3` and `python3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final words\n",
    "As you probably already know, code has to be written in the grey cells. You execute a cell by clicking the **Run**-button (or **Ctrl + Enter**). If you want to run all cells of the notebook at once, click **Cell > Run All** or **Kernel > Restart & Run All** respectively, if you want to restart the Python kernel first. On the left side of an (unexecuted) cell stands `In [ ]:`. The empty bracket means, that the cell hasn't been executed yet. By clicking **Run**, a star appears in the brackets (`In [*]:`), which means the process is running. In most cases, you won't see that star, because your computer is faster than your eyes. You can execute only one cell at once, all following executions will be in the waiting line. If the process of a cell is done, a number appears in the brackets (`In [1]:`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Starting with topic modeling!\n",
    "\n",
    "Execute the following cell to import modules from the `dariah_topics` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from cophi_toolbox import preprocessing\n",
    "from dariah_topics import utils\n",
    "from dariah_topics import postprocessing\n",
    "from dariah_topics import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will need some additional functions from external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metadata_toolbox.utils as metadata\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not pay heed to any warnings right now and execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Reading a corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the path to the corpus folder\n",
    "\n",
    "In the present example code, we are using the 30 diary excerpts from the folder `grenzboten`. To use your own corpus, change the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = Path('data', 'grenzboten_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the pattern of filenames for metadata extraction\n",
    "\n",
    "You have the ability to extract metadata from the filenames. For instance, if your textfiles look like:\n",
    "\n",
    "```\n",
    "goethe_1816_stella.txt\n",
    "```\n",
    "\n",
    "the pattern would look like this:\n",
    "\n",
    "```\n",
    "{author}_{year}_{title}\n",
    "```\n",
    "\n",
    "So, let's try this for the example corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '{author}_{year}_{title}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing file paths and metadata\n",
    "We begin by creating a list of all the documents in the folder specified above. That list will tell the function `preprocessing.read_files` (see below) which text documents to read. Furthermore, based on filenames we can create some metadata, e.g. author and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Beck_1844_Tagebuch_56.txt</th>\n",
       "      <td>Beck</td>\n",
       "      <td>1844</td>\n",
       "      <td>Tagebuch_56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Berto_1915_Kriegstagebuch_94.txt</th>\n",
       "      <td>Berto</td>\n",
       "      <td>1915</td>\n",
       "      <td>Kriegstagebuch_94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Castelli_1846_Tagebuch_51.txt</th>\n",
       "      <td>Castelli</td>\n",
       "      <td>1846</td>\n",
       "      <td>Tagebuch_51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Cleinom_1914_Kriegstagebuch_94.txt</th>\n",
       "      <td>Cleinom</td>\n",
       "      <td>1914</td>\n",
       "      <td>Kriegstagebuch_94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\grenzboten_sample\\Dix_1914_Kriegstagebuch_37.txt</th>\n",
       "      <td>Dix</td>\n",
       "      <td>1914</td>\n",
       "      <td>Kriegstagebuch_37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      author  year  \\\n",
       "data\\grenzboten_sample\\Beck_1844_Tagebuch_56.txt        Beck  1844   \n",
       "data\\grenzboten_sample\\Berto_1915_Kriegstagebuc...     Berto  1915   \n",
       "data\\grenzboten_sample\\Castelli_1846_Tagebuch_5...  Castelli  1846   \n",
       "data\\grenzboten_sample\\Cleinom_1914_Kriegstageb...   Cleinom  1914   \n",
       "data\\grenzboten_sample\\Dix_1914_Kriegstagebuch_...       Dix  1914   \n",
       "\n",
       "                                                                title  \n",
       "data\\grenzboten_sample\\Beck_1844_Tagebuch_56.txt          Tagebuch_56  \n",
       "data\\grenzboten_sample\\Berto_1915_Kriegstagebuc...  Kriegstagebuch_94  \n",
       "data\\grenzboten_sample\\Castelli_1846_Tagebuch_5...        Tagebuch_51  \n",
       "data\\grenzboten_sample\\Cleinom_1914_Kriegstageb...  Kriegstagebuch_94  \n",
       "data\\grenzboten_sample\\Dix_1914_Kriegstagebuch_...  Kriegstagebuch_37  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.concat([metadata.fname2metadata(str(path), pattern=pattern) for path in path_to_corpus.glob('*.txt')])\n",
    "meta[:5] # by adding '[:5]' to the variable, only the first 5 elements will be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read listed documents from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tagebuch von Karl Beck. Man spricht seit vierzehn Tagen von einem vollstÃ¤ndigen Ministerwechsel und es circuliren im Publicum die verschiedensten Combinationen, wobei heute ganz andere Namen genannt werden, als gestern und morgen wieder andere, als heute.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(preprocessing.read_files(meta.index))\n",
    "corpus[0][:255] # printing the first 255 characters of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `corpus` contains as much elements (`documents`) as texts in your corpus are. Each element of `corpus` is a list containing exactly one element, the text itself as one single string including all whitespaces and punctuations:\n",
    "\n",
    "```\n",
    "[['This is the content of your first document.'],\n",
    " ['This is the content of your second document.'],\n",
    " ...\n",
    " ['This is the content of your last document.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tokenize corpus\n",
    "Now, your `documents` in `corpus` will be tokenized. Tokenization is the task of cutting a stream of characters into linguistic units, simply words or, more precisely, tokens. The tokenize function `dariah_topics` provides is a simple Unicode tokenizer. Depending on the corpus, it might be useful to use an external tokenizer function, or even develop your own, since its efficiency varies with language, epoch and text type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [list(preprocessing.tokenize(document)) for document in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, each `document` is represented by a list of separate token strings. As above, have a look at the first document (which has the index `0` as Python starts counting at 0) and show its first 14 words/tokens (that have the indices `0:13` accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagebuch',\n",
       " 'von',\n",
       " 'karl',\n",
       " 'beck',\n",
       " 'man',\n",
       " 'spricht',\n",
       " 'seit',\n",
       " 'vierzehn',\n",
       " 'tagen',\n",
       " 'von',\n",
       " 'einem',\n",
       " 'vollstÃ¤ndigen',\n",
       " 'ministerwechsel']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[0][0:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create a document-term matrix\n",
    "\n",
    "The LDA topic model is based on a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) of the corpus. To improve performance in large corpora, the matrix describes the frequency of terms that occur in the collection. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Large corpus matrix\n",
    "\n",
    "If you have a very large corpus, create a document-term matrix designed for large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                       0\n",
       " document_id type_id     \n",
       " 1           1075      38\n",
       "             2331       1\n",
       "             17641      3\n",
       "             3240       1\n",
       "             2912       5\n",
       "             14100    128\n",
       "             11231      1\n",
       "             13599    169\n",
       "             7352       2\n",
       "             7617     207\n",
       "             4969       1\n",
       "             14887      1\n",
       "             19113    213\n",
       "             4772       4\n",
       "             6927       2\n",
       "             3529       2\n",
       "             8019       2\n",
       "             22750      1\n",
       "             18600      1\n",
       "             20043      1\n",
       "             23954      3\n",
       "             7845       2\n",
       "             15929      1\n",
       "             9359       1\n",
       "             11972      2\n",
       "             1569      31\n",
       "             20302     23\n",
       "             3051       2\n",
       "             23720      1\n",
       "             16954     14\n",
       " ...                  ...\n",
       " 27          8249       1\n",
       "             20954      1\n",
       "             16906      1\n",
       "             510        1\n",
       "             20483      1\n",
       "             18692      1\n",
       "             9012       1\n",
       "             6647       1\n",
       "             10405      1\n",
       "             10239      1\n",
       "             15338      1\n",
       "             11796      1\n",
       "             9562       1\n",
       "             1138       1\n",
       "             11924      1\n",
       "             5817       1\n",
       "             23519      1\n",
       "             1267       1\n",
       "             15143      1\n",
       "             6956       1\n",
       "             22205      1\n",
       "             18781      1\n",
       "             2878       1\n",
       "             8553       1\n",
       "             18869      1\n",
       "             15866      1\n",
       "             991        1\n",
       "             7779       1\n",
       "             6059       1\n",
       "             2430       1\n",
       " \n",
       " [46297 rows x 1 columns],\n",
       " {'Tagebuch_82': 1,\n",
       "  'Tagebuch_93': 2,\n",
       "  'Kriegstagebuch_95': 3,\n",
       "  'Kriegstagebuch_48': 4,\n",
       "  'Tagebuch_77': 5,\n",
       "  'Tagebuch_56': 6,\n",
       "  'Tagebuch_51': 7,\n",
       "  'Kriegstagebuch_69': 8,\n",
       "  'Kriegstagebuch_97': 9,\n",
       "  'Kriegstagebuch_37': 10,\n",
       "  'Tagebuch_72': 11,\n",
       "  'Kriegstagebuch_99': 12,\n",
       "  'Tagebuch_88': 13,\n",
       "  'Tagebuch_81': 14,\n",
       "  'Tagebuch_85': 15,\n",
       "  'Kriegstagebuch_81': 16,\n",
       "  'Kriegstagebuch_94': 17,\n",
       "  'Kriegstagebuch_68': 18,\n",
       "  'Tagebuch_52': 19,\n",
       "  'Tagebuch_96': 20,\n",
       "  'Kriegstagebuch_39': 21,\n",
       "  'Kriegstagebuch_41': 22,\n",
       "  'Tagebuch_70': 23,\n",
       "  'Kriegstagebuch_49': 24,\n",
       "  'Tagebuch_62': 25,\n",
       "  'Kriegstagebuch_73': 26,\n",
       "  'Kriegstagebuch_33': 27},\n",
       " {'wartete': 1,\n",
       "  'guter': 2,\n",
       "  'schwÃ¼l': 3,\n",
       "  'befehl': 4,\n",
       "  'elferspitze': 5,\n",
       "  'unsittlichkeit': 6,\n",
       "  'einnistet': 7,\n",
       "  'v-rlags': 8,\n",
       "  'zonguldak': 9,\n",
       "  'vortheilhaftes': 10,\n",
       "  'sauhirt': 11,\n",
       "  'ungÃ¼nstigsten': 12,\n",
       "  'fee-mad': 13,\n",
       "  'admiral': 14,\n",
       "  'wortes': 15,\n",
       "  'frauen-akademie': 16,\n",
       "  'ta': 17,\n",
       "  'appanv': 18,\n",
       "  'gewiÃŸ': 19,\n",
       "  'meere': 20,\n",
       "  'scheinenden': 21,\n",
       "  'eteruftrasie': 22,\n",
       "  'maroccaner': 23,\n",
       "  'schwammen': 24,\n",
       "  'protestirt': 25,\n",
       "  'zÃ¤rtlichkeit': 26,\n",
       "  'steinbach': 27,\n",
       "  'gefahrvoller': 28,\n",
       "  'gevurtsscstes': 29,\n",
       "  'gedankenscheue': 30,\n",
       "  'deutschpolen': 31,\n",
       "  'hineinbesorgen': 32,\n",
       "  'bÃ¤der': 33,\n",
       "  'nÃ¶rdlich': 34,\n",
       "  'auerbach': 35,\n",
       "  'unfreier': 36,\n",
       "  'capitains': 37,\n",
       "  'eigen': 38,\n",
       "  'actionen': 39,\n",
       "  'mÃ¼ÃŸt': 40,\n",
       "  'wirksamkeit': 41,\n",
       "  'fetzen': 42,\n",
       "  'auszuÃ¼ben': 43,\n",
       "  'rothe': 44,\n",
       "  'officieller': 45,\n",
       "  'devise': 46,\n",
       "  'insurgenten': 47,\n",
       "  'sculpcuren': 48,\n",
       "  'gemachte': 49,\n",
       "  'hinausgeht': 50,\n",
       "  'aufgeschwornengcrichte': 51,\n",
       "  'voraussetzliche': 52,\n",
       "  'adlerprinzen': 53,\n",
       "  'decennien': 54,\n",
       "  'muselmÃ¤nnische': 55,\n",
       "  'heftiger': 56,\n",
       "  'hannover': 57,\n",
       "  'theuer': 58,\n",
       "  'forscher': 59,\n",
       "  'fÃ¤ulniÃŸ': 60,\n",
       "  'schrift': 61,\n",
       "  'kopfnicken': 62,\n",
       "  'abscheulichen': 63,\n",
       "  'bekrittelt': 64,\n",
       "  'ehrenmÃ¤nner': 65,\n",
       "  'gemeiner': 66,\n",
       "  'feldlager': 67,\n",
       "  'schwanken': 68,\n",
       "  'gleich': 69,\n",
       "  'a?beiter': 70,\n",
       "  'octo': 71,\n",
       "  'landwirtschaftsverwaltung': 72,\n",
       "  'verlÃ¤ÃŸt': 73,\n",
       "  'nordwestlichen': 74,\n",
       "  'briren': 75,\n",
       "  'historisches': 76,\n",
       "  'deutschrechtlichen': 77,\n",
       "  'gesamtzahl': 78,\n",
       "  \"vauschke's\": 79,\n",
       "  'schneidergesellen': 80,\n",
       "  'wÃ¼rdest': 81,\n",
       "  'buches': 82,\n",
       "  'jÂ»i': 83,\n",
       "  'anlangt': 84,\n",
       "  'manifestation': 85,\n",
       "  'borstÃ¶ÃŸe': 86,\n",
       "  'kanal': 87,\n",
       "  'gespalten': 88,\n",
       "  'mentiÃ¶res': 89,\n",
       "  'dunnjec': 90,\n",
       "  'vierte': 91,\n",
       "  'besteuerung': 92,\n",
       "  'bedeutung': 93,\n",
       "  'llicu': 94,\n",
       "  'klagbar': 95,\n",
       "  'anzeigt': 96,\n",
       "  'arretirt': 97,\n",
       "  'aufwÃ¤rts': 98,\n",
       "  'dobrcvÃ¼ski': 99,\n",
       "  'manullriptscndnngen': 100,\n",
       "  'dificiren': 101,\n",
       "  'Ã¼bergesiedelt': 102,\n",
       "  'rob': 103,\n",
       "  'anzukaufen': 104,\n",
       "  'flibustier': 105,\n",
       "  'rheine': 106,\n",
       "  'epossen': 107,\n",
       "  'teuflischen': 108,\n",
       "  'prognostikon': 109,\n",
       "  'Ã¼berrascht': 110,\n",
       "  'instrumentalsten': 111,\n",
       "  'manz': 112,\n",
       "  'reaction': 113,\n",
       "  'ezechen': 114,\n",
       "  'turm': 115,\n",
       "  'reibungen': 116,\n",
       "  'zurief': 117,\n",
       "  'widerwÃ¤rtigen': 118,\n",
       "  'deutlicher': 119,\n",
       "  'mautbeamten': 120,\n",
       "  'handstreich': 121,\n",
       "  'saale': 122,\n",
       "  'warnungen': 123,\n",
       "  'wohlgefallen': 124,\n",
       "  'jnvincible': 125,\n",
       "  'starre': 126,\n",
       "  'castle': 127,\n",
       "  'tage.spater': 128,\n",
       "  'zerwÃ¼rfniÃŸ': 129,\n",
       "  'classe': 130,\n",
       "  'hierarchie': 131,\n",
       "  'kameeltreiber': 132,\n",
       "  'angebornen': 133,\n",
       "  'grÃ¶ÃŸen': 134,\n",
       "  'haidcschenke': 135,\n",
       "  'geduckt': 136,\n",
       "  'bibliotheks': 137,\n",
       "  'sonderbare': 138,\n",
       "  'gedrungen': 139,\n",
       "  'vermochte': 140,\n",
       "  'leitet': 141,\n",
       "  'bÃ¼rgerwohl': 142,\n",
       "  'mannschaft': 143,\n",
       "  'nordÃ¶stl': 144,\n",
       "  'suchten': 145,\n",
       "  'gas': 146,\n",
       "  'irdischen': 147,\n",
       "  'unbefangenen': 148,\n",
       "  'tabaksbau': 149,\n",
       "  'jalousie': 150,\n",
       "  'ordinÃ¤res': 151,\n",
       "  'ausgebildeten': 152,\n",
       "  'barkhat-oase': 153,\n",
       "  'mazedonien': 154,\n",
       "  'ipern': 155,\n",
       "  'allerlei': 156,\n",
       "  'niebuhrs': 157,\n",
       "  'wurzelt': 158,\n",
       "  'glÃ¤ubige': 159,\n",
       "  'baute': 160,\n",
       "  'liefert': 161,\n",
       "  'elite': 162,\n",
       "  'goldene': 163,\n",
       "  'bekehren': 164,\n",
       "  'gedachter': 165,\n",
       "  'belgien': 166,\n",
       "  'eiern': 167,\n",
       "  'reisegelegenheiten': 168,\n",
       "  'anklÃ¤gern': 169,\n",
       "  'andachtsÃ¼bungen': 170,\n",
       "  'legitimistischcr': 171,\n",
       "  'thÃ¤tigen': 172,\n",
       "  'aufsÃ¤he': 173,\n",
       "  'freude': 174,\n",
       "  'zuspricht': 175,\n",
       "  'mobilisierungsvorbereitungen': 176,\n",
       "  'werftanlagen': 177,\n",
       "  'industriezweig': 178,\n",
       "  'hausbesitzer': 179,\n",
       "  'peking': 180,\n",
       "  'beiÃŸenden': 181,\n",
       "  'ungerecht': 182,\n",
       "  'unterhandeln': 183,\n",
       "  'finsterlinge': 184,\n",
       "  'privatbesitze': 185,\n",
       "  'sperrung': 186,\n",
       "  'marburg': 187,\n",
       "  'schwein': 188,\n",
       "  'kampfbegierigm': 189,\n",
       "  'aichy': 190,\n",
       "  'anspielungen': 191,\n",
       "  'auslÃ¤ndischen': 192,\n",
       "  'wohlthÃ¤tigkeitssinn': 193,\n",
       "  'reformers': 194,\n",
       "  'Ã¤ltern': 195,\n",
       "  'gelegene': 196,\n",
       "  'wohlunterrichteter': 197,\n",
       "  'abschlie': 198,\n",
       "  'herd': 199,\n",
       "  'verwerfung': 200,\n",
       "  'sÃ¼dlÃ¤ndisches': 201,\n",
       "  'vaterlÃ¤ndischen': 202,\n",
       "  'lachte': 203,\n",
       "  'kunstsinn': 204,\n",
       "  'proben': 205,\n",
       "  'maus': 206,\n",
       "  'besitzen': 207,\n",
       "  'reichstages': 208,\n",
       "  'befindet': 209,\n",
       "  'grÃ¶ÃŸerm': 210,\n",
       "  'musicirt': 211,\n",
       "  'attentat': 212,\n",
       "  'miÃŸgeschicke': 213,\n",
       "  'tractate': 214,\n",
       "  'hervorschlagen': 215,\n",
       "  'blemm': 216,\n",
       "  'berlag': 217,\n",
       "  'intendanten': 218,\n",
       "  'aufzufordern': 219,\n",
       "  'obcrcensurgerichts': 220,\n",
       "  'john': 221,\n",
       "  'eingehÃ¤ndigt': 222,\n",
       "  'iÂ»ri': 223,\n",
       "  'echt': 224,\n",
       "  'deutsch-sÃ¼dwestafrikanische': 225,\n",
       "  'Ã¼brigen': 226,\n",
       "  'fiscalaction': 227,\n",
       "  'styr-kme': 228,\n",
       "  'fahrt': 229,\n",
       "  'raton': 230,\n",
       "  'breter': 231,\n",
       "  'vorurtheilen': 232,\n",
       "  'zwei': 233,\n",
       "  'gclehrte': 234,\n",
       "  'zahlt': 235,\n",
       "  'concessionen': 236,\n",
       "  'courier': 237,\n",
       "  'freih': 238,\n",
       "  'solde': 239,\n",
       "  'etwaniger': 240,\n",
       "  'dorf': 241,\n",
       "  'baumeister': 242,\n",
       "  'pathos': 243,\n",
       "  'gÃ¼nstige': 244,\n",
       "  'statthalter': 245,\n",
       "  'ernsthaft': 246,\n",
       "  'colonisten': 247,\n",
       "  'vÃ¶lligen': 248,\n",
       "  'pitoyablen': 249,\n",
       "  'ministerkrisen': 250,\n",
       "  'gekonnt': 251,\n",
       "  'geier': 252,\n",
       "  'nehmlich': 253,\n",
       "  'erclusivsten': 254,\n",
       "  'berichtigungen': 255,\n",
       "  'stÃ¤tten': 256,\n",
       "  'verwirklichen': 257,\n",
       "  'redner': 258,\n",
       "  'angeschlossen': 259,\n",
       "  'verstÃ¤ndlichen': 260,\n",
       "  'handwerker': 261,\n",
       "  'randfontein': 262,\n",
       "  'wohldressirten': 263,\n",
       "  'vorgegangen': 264,\n",
       "  'umschwung': 265,\n",
       "  'eingedrungenen': 266,\n",
       "  'auszukÃ¤mpfen': 267,\n",
       "  'eclatant': 268,\n",
       "  'conventionsmÃ¼nze': 269,\n",
       "  'theaterverhÃ¤ltnisse': 270,\n",
       "  'meisten': 271,\n",
       "  'Ã¼bereinstimmen': 272,\n",
       "  \"gÃ¶the'schen\": 273,\n",
       "  'schauspielev': 274,\n",
       "  'szittkehmen': 275,\n",
       "  'tartarenstÃ¤mmen': 276,\n",
       "  'czeremosz': 277,\n",
       "  'ehrenlegion': 278,\n",
       "  'comitat': 279,\n",
       "  'liebeshandel': 280,\n",
       "  'tausenden': 281,\n",
       "  'untersuchungen': 282,\n",
       "  'preuÃŸisches': 283,\n",
       "  'verzichtet': 284,\n",
       "  'ergreifend': 285,\n",
       "  'bedingt': 286,\n",
       "  'unglÃ¼ckselig': 287,\n",
       "  'stÃ¼nde': 288,\n",
       "  'vvritv': 289,\n",
       "  'anwendete': 290,\n",
       "  'knospen': 291,\n",
       "  'heldenkÃ¤mpfen': 292,\n",
       "  'begrÃ¼ndet': 293,\n",
       "  'ganz': 294,\n",
       "  'cir': 295,\n",
       "  'erstatten': 296,\n",
       "  'verwandte': 297,\n",
       "  'baumgruppen': 298,\n",
       "  'wizniowczik': 299,\n",
       "  'ruÃŸlands': 300,\n",
       "  'landstÃ¤nde': 301,\n",
       "  'nichtgemeingefahrlichkeits-standpunkt': 302,\n",
       "  'dÃ¤nisch': 303,\n",
       "  'lumpereien': 304,\n",
       "  'staatsgewalt': 305,\n",
       "  'konfiszierte': 306,\n",
       "  'letzterem': 307,\n",
       "  'schlicht': 308,\n",
       "  'gotthold': 309,\n",
       "  'verbesserten': 310,\n",
       "  'wÃ¶nigcr': 311,\n",
       "  'gegenstoÃŸ': 312,\n",
       "  'aufmerksam': 313,\n",
       "  'maule': 314,\n",
       "  'passirt': 315,\n",
       "  'gesammtwohl': 316,\n",
       "  'sse': 317,\n",
       "  'bettlermantel': 318,\n",
       "  'baucommission': 319,\n",
       "  'hinzuzufÃ¼gen': 320,\n",
       "  'd-sjcnier': 321,\n",
       "  'octbr': 322,\n",
       "  'stÃ¤rken': 323,\n",
       "  'edlerer': 324,\n",
       "  'zugenommen': 325,\n",
       "  \"will's\": 326,\n",
       "  'pilze': 327,\n",
       "  'fortgespielt': 328,\n",
       "  'schifflein': 329,\n",
       "  'erdrÃ¼': 330,\n",
       "  'dessau-r': 331,\n",
       "  'vermÃ¤hlen': 332,\n",
       "  'pÃ¶belausschreitungen': 333,\n",
       "  'hoftag': 334,\n",
       "  'abdul': 335,\n",
       "  'provin': 336,\n",
       "  'durchaus': 337,\n",
       "  'dreizehntes': 338,\n",
       "  'neuntÃ¤gige': 339,\n",
       "  'trÃ¼mmer': 340,\n",
       "  'enthÃ¼llung': 341,\n",
       "  'pfusche': 342,\n",
       "  'vorzustellen': 343,\n",
       "  'jungen': 344,\n",
       "  'tage': 345,\n",
       "  'betrachten': 346,\n",
       "  'Ã¼bersteigt': 347,\n",
       "  'allianz': 348,\n",
       "  'fortan': 349,\n",
       "  'pensum': 350,\n",
       "  'einzelnfÃ¤lle': 351,\n",
       "  'oerstedt': 352,\n",
       "  'berlin-frieden': 353,\n",
       "  'siegesmeldungen': 354,\n",
       "  'nachtwÃ¤chter': 355,\n",
       "  'menagerie': 356,\n",
       "  'stallknecht': 357,\n",
       "  'bearbeitet': 358,\n",
       "  'standemitglied': 359,\n",
       "  'lampenschimmer': 360,\n",
       "  'danken': 361,\n",
       "  'achtfacher': 362,\n",
       "  'warnt': 363,\n",
       "  'vorbehalten': 364,\n",
       "  'planmÃ¤ÃŸig': 365,\n",
       "  'steinerne': 366,\n",
       "  'wmv': 367,\n",
       "  'apodiktische': 368,\n",
       "  'allein': 369,\n",
       "  'treu': 370,\n",
       "  'ganzen': 371,\n",
       "  'runctort': 372,\n",
       "  'bruchstÃ¼ck': 373,\n",
       "  \"weber's\": 374,\n",
       "  'einfÃ¼hrung': 375,\n",
       "  'betrachtet': 376,\n",
       "  'materialistischen': 377,\n",
       "  'heimisch': 378,\n",
       "  'zurÃ¼cktreten': 379,\n",
       "  'wohlbekannte': 380,\n",
       "  'plagiatsanklage': 381,\n",
       "  'dendermonde': 382,\n",
       "  'entschiedenste': 383,\n",
       "  'kraljevoâ€”trotenik': 384,\n",
       "  'fuÃŸtritt': 385,\n",
       "  'urthelsprechung': 386,\n",
       "  'stapelplatzes': 387,\n",
       "  'setzende': 388,\n",
       "  'lÃ¤ÃŸt': 389,\n",
       "  'catharine': 390,\n",
       "  'torystischen': 391,\n",
       "  'borschuÃŸ': 392,\n",
       "  'Ã¼brige': 393,\n",
       "  'anfrage': 394,\n",
       "  'lamartine': 395,\n",
       "  'plateau': 396,\n",
       "  'prÃ¤chtigeren': 397,\n",
       "  'geldinstitute': 398,\n",
       "  'scheve': 399,\n",
       "  'kÃ¶nigliche': 400,\n",
       "  'mÃ¼nchner': 401,\n",
       "  'postillon': 402,\n",
       "  'machwerk': 403,\n",
       "  'mis': 404,\n",
       "  'nero': 405,\n",
       "  'artilieneschÃ¼ler': 406,\n",
       "  'argumente': 407,\n",
       "  'oberkommando': 408,\n",
       "  'lemerle': 409,\n",
       "  'bombastische': 410,\n",
       "  'vorpostenboote': 411,\n",
       "  'guirt': 412,\n",
       "  'hinter': 413,\n",
       "  'religionsbekenntnisses': 414,\n",
       "  'cengio': 415,\n",
       "  'eorrespondenten': 416,\n",
       "  'schmerzliches': 417,\n",
       "  'legt': 418,\n",
       "  'confrontirt': 419,\n",
       "  'angst': 420,\n",
       "  'westfront': 421,\n",
       "  \"austernl'erge\": 422,\n",
       "  'aufgaben': 423,\n",
       "  'individuelle': 424,\n",
       "  'verwÃ¼nscht': 425,\n",
       "  'gt;i': 426,\n",
       "  'pfaffenmacht': 427,\n",
       "  'abschiede': 428,\n",
       "  'verlag': 429,\n",
       "  'tartaren': 430,\n",
       "  'kunstgerechten': 431,\n",
       "  'bcct': 432,\n",
       "  'brzeziny': 433,\n",
       "  'schauspielerin': 434,\n",
       "  'zinne': 435,\n",
       "  'ncpumukfcst': 436,\n",
       "  'scharfblicke': 437,\n",
       "  'tractatmÃ¤ÃŸig': 438,\n",
       "  'gelegt': 439,\n",
       "  'erquickungsstunden': 440,\n",
       "  \"law'schen\": 441,\n",
       "  'si.im': 442,\n",
       "  'aristokratischen': 443,\n",
       "  'land': 444,\n",
       "  'ersteres': 445,\n",
       "  'schwedische': 446,\n",
       "  'abbas': 447,\n",
       "  'passenden': 448,\n",
       "  'splendider': 449,\n",
       "  'offizielle': 450,\n",
       "  'journalistische': 451,\n",
       "  'wackerste': 452,\n",
       "  'stobychwa': 453,\n",
       "  'frischen': 454,\n",
       "  'nordamerikanische': 455,\n",
       "  'eÂ«s': 456,\n",
       "  'zerschossene': 457,\n",
       "  'jungfrauen': 458,\n",
       "  'schlimmste': 459,\n",
       "  'wehmÃ¼thigen': 460,\n",
       "  'offenen': 461,\n",
       "  'busibollo': 462,\n",
       "  'klippe': 463,\n",
       "  'hertÃ¶nende': 464,\n",
       "  'sdrsckttcher': 465,\n",
       "  'bemÃ¤kelt': 466,\n",
       "  'jrakfront': 467,\n",
       "  'zeuge': 468,\n",
       "  'adressiren': 469,\n",
       "  'abgereist': 470,\n",
       "  'tÃ¶pfer': 471,\n",
       "  'luftangriff': 472,\n",
       "  'bourbon': 473,\n",
       "  'verbot': 474,\n",
       "  'schlechten': 475,\n",
       "  'hierhergehÃ¶rige': 476,\n",
       "  'schwall': 477,\n",
       "  'beginn': 478,\n",
       "  'i.emol': 479,\n",
       "  'wahrer': 480,\n",
       "  'aufruf': 481,\n",
       "  'religiÃ¶sem': 482,\n",
       "  'gÃ¶rzer': 483,\n",
       "  'dreyschock': 484,\n",
       "  'gesichte': 485,\n",
       "  'erfreute': 486,\n",
       "  'erfreut': 487,\n",
       "  'vorbildung': 488,\n",
       "  'baccarola': 489,\n",
       "  'proceÃŸordnung': 490,\n",
       "  'autors': 491,\n",
       "  'beizutragen': 492,\n",
       "  'aufgefordert': 493,\n",
       "  'erfahrungen': 494,\n",
       "  'etsch': 495,\n",
       "  'desgl': 496,\n",
       "  'jÃ¼tschen': 497,\n",
       "  'voltaire': 498,\n",
       "  'ehrgeizige': 499,\n",
       "  'verspÃ¤teter': 500,\n",
       "  'baumstamme': 501,\n",
       "  'bezeichnet': 502,\n",
       "  'abgeschmacktesten': 503,\n",
       "  'heirathsschlieÃŸung': 504,\n",
       "  'gedichtes': 505,\n",
       "  'n-ichs': 506,\n",
       "  'verurteilten': 507,\n",
       "  'genossenschaften': 508,\n",
       "  'schritte': 509,\n",
       "  'ausdrÃ¼cklicher': 510,\n",
       "  'abgibt': 511,\n",
       "  'eingeÃ¼bt': 512,\n",
       "  'noiddeuische': 513,\n",
       "  'bemÃ¼hte': 514,\n",
       "  'fci': 515,\n",
       "  'feuilletonartikel': 516,\n",
       "  'abenden': 517,\n",
       "  \"in's\": 518,\n",
       "  'fÃ¼llen': 519,\n",
       "  'buchhandlung': 520,\n",
       "  'lebensvolles': 521,\n",
       "  'katho': 522,\n",
       "  'liederlichkeit': 523,\n",
       "  'keller': 524,\n",
       "  'kenntniÃŸ': 525,\n",
       "  'unmÃ¤ÃŸig': 526,\n",
       "  'wilhelms': 527,\n",
       "  'campomolon': 528,\n",
       "  'ortkn': 529,\n",
       "  'definition': 530,\n",
       "  'elegant': 531,\n",
       "  'Ã¤o': 532,\n",
       "  'caprile': 533,\n",
       "  'nÃ¤hernde': 534,\n",
       "  'einfÃ¼hren': 535,\n",
       "  \"subjectiven'gefÃ¼hls\": 536,\n",
       "  'frisch': 537,\n",
       "  'fleisch': 538,\n",
       "  'beim': 539,\n",
       "  'unverhÃ¤ltniÃŸmÃ¤ÃŸige': 540,\n",
       "  'esprit': 541,\n",
       "  'nuancen': 542,\n",
       "  'hofequipagen': 543,\n",
       "  'prachtbau': 544,\n",
       "  'laune': 545,\n",
       "  'geschriebenen': 546,\n",
       "  'britischen': 547,\n",
       "  'vlaemen': 548,\n",
       "  'krÃ¤chzen': 549,\n",
       "  'gelbveiglein': 550,\n",
       "  'gelten': 551,\n",
       "  'signora': 552,\n",
       "  'bekriegt': 553,\n",
       "  'geistliche': 554,\n",
       "  'endigte': 555,\n",
       "  'haushoch': 556,\n",
       "  'locale': 557,\n",
       "  'sonders': 558,\n",
       "  'holatyn-grn': 559,\n",
       "  'soirÃ¶en': 560,\n",
       "  'zeppelin-luftschiffes': 561,\n",
       "  'kriegsmacht': 562,\n",
       "  'rouge': 563,\n",
       "  'regie': 564,\n",
       "  'reprÃ¤sentant': 565,\n",
       "  'gelebt': 566,\n",
       "  'anzuerkennendes': 567,\n",
       "  'ertheilung': 568,\n",
       "  'tatkraft': 569,\n",
       "  'urwÃ¤ldern': 570,\n",
       "  'weser-zeitung': 571,\n",
       "  'pÃ¤dagogischen': 572,\n",
       "  'bortheil': 573,\n",
       "  'anonymen': 574,\n",
       "  'craonner': 575,\n",
       "  'ungeschickter': 576,\n",
       "  'sphÃ¤re': 577,\n",
       "  'straÃŸenkampfe': 578,\n",
       "  'fÃ¼nfzehn': 579,\n",
       "  'volksthum': 580,\n",
       "  'bitterkeit': 581,\n",
       "  'abberufung': 582,\n",
       "  'vernichtet': 583,\n",
       "  'volksbildung': 584,\n",
       "  'stÃ¤rker': 585,\n",
       "  'kÃ¼nstlerleben': 586,\n",
       "  'rechts': 587,\n",
       "  'toir': 588,\n",
       "  'quintett': 589,\n",
       "  'waarenballen': 590,\n",
       "  'klub': 591,\n",
       "  'direkt': 592,\n",
       "  'tadeln': 593,\n",
       "  'stÃ¼rmt': 594,\n",
       "  'menschenrassen': 595,\n",
       "  'redacteure': 596,\n",
       "  'verpfÃ¤ndete': 597,\n",
       "  'stadttheater': 598,\n",
       "  'stockerau': 599,\n",
       "  'mal': 600,\n",
       "  'skeptisch': 601,\n",
       "  'versenkt': 602,\n",
       "  'frÃ¼her': 603,\n",
       "  'bereitwillig': 604,\n",
       "  'erpressen': 605,\n",
       "  'notengewandtheit': 606,\n",
       "  'durchlebt': 607,\n",
       "  'ewiger': 608,\n",
       "  'beruf': 609,\n",
       "  'achselzuckend': 610,\n",
       "  'eisbrecher': 611,\n",
       "  'paital': 612,\n",
       "  'inhaber': 613,\n",
       "  'sextett': 614,\n",
       "  'police': 615,\n",
       "  'orleans': 616,\n",
       "  'socialistisches': 617,\n",
       "  'melles': 618,\n",
       "  'kongostaates': 619,\n",
       "  'abgenommenen': 620,\n",
       "  'anreiht': 621,\n",
       "  'artillerie-park': 622,\n",
       "  'sonn': 623,\n",
       "  'beraubt': 624,\n",
       "  'versallen': 625,\n",
       "  'aberstatt': 626,\n",
       "  'christkatholicismus': 627,\n",
       "  'prÃ¤tendent': 628,\n",
       "  'armee': 629,\n",
       "  'anregungen': 630,\n",
       "  'knochen': 631,\n",
       "  'erreicht': 632,\n",
       "  'emeri': 633,\n",
       "  'familien': 634,\n",
       "  'vorgehend': 635,\n",
       "  'hilfskreuzer': 636,\n",
       "  'damens': 637,\n",
       "  'sind': 638,\n",
       "  'advocaten': 639,\n",
       "  'eiragola': 640,\n",
       "  'congreÃŸacte': 641,\n",
       "  'mannheim': 642,\n",
       "  'abscheu': 643,\n",
       "  'endlose': 644,\n",
       "  'gast': 645,\n",
       "  'l.alter': 646,\n",
       "  'herabgeschossen': 647,\n",
       "  'wiederholt': 648,\n",
       "  'aufliegt': 649,\n",
       "  'getreide': 650,\n",
       "  'wandten': 651,\n",
       "  'ununterbrochene': 652,\n",
       "  'hlubeck': 653,\n",
       "  'bewÃ¤ltigen': 654,\n",
       "  \"m'n\": 655,\n",
       "  'west-yorkshire': 656,\n",
       "  'nationale': 657,\n",
       "  'langes': 658,\n",
       "  'ni-dovimt': 659,\n",
       "  'lescvcrein': 660,\n",
       "  'aufdeckt': 661,\n",
       "  'dra': 662,\n",
       "  'gefangenen': 663,\n",
       "  'nahe': 664,\n",
       "  'pfeifen': 665,\n",
       "  'peter': 666,\n",
       "  \"stÃ¤del'schen\": 667,\n",
       "  \"ob's\": 668,\n",
       "  'bordeaux': 669,\n",
       "  'umfangreicher': 670,\n",
       "  'dennoch': 671,\n",
       "  'vergilbten': 672,\n",
       "  'unterhalt': 673,\n",
       "  'concilium': 674,\n",
       "  'beleidigt': 675,\n",
       "  'schÃ¶pfer': 676,\n",
       "  'polyp': 677,\n",
       "  'verunglÃ¼cktes': 678,\n",
       "  'mercantilischen': 679,\n",
       "  'persische': 680,\n",
       "  'stÃ¼rmen': 681,\n",
       "  'glenzbotcn': 682,\n",
       "  'kÃ¼ssen': 683,\n",
       "  'gewohnt': 684,\n",
       "  'etabisse': 685,\n",
       "  'schlieÃŸen': 686,\n",
       "  'midille': 687,\n",
       "  'entlehnen': 688,\n",
       "  'hÃ¤ÃŸlichsten': 689,\n",
       "  'zwwm': 690,\n",
       "  'lÃ¤ngere': 691,\n",
       "  'mÃ¤dchens': 692,\n",
       "  'augenweide': 693,\n",
       "  'neben': 694,\n",
       "  'vice': 695,\n",
       "  'thalia-theaters': 696,\n",
       "  'unverkennbar': 697,\n",
       "  'betrÃ¼gerischen': 698,\n",
       "  'vermÃ¤hlung': 699,\n",
       "  'fÃ¼rwahr': 700,\n",
       "  'dringend': 701,\n",
       "  'wohlthaten': 702,\n",
       "  'eichenlaub': 703,\n",
       "  'getÃ¶tet': 704,\n",
       "  'haltbarste': 705,\n",
       "  'tragen': 706,\n",
       "  'kantara': 707,\n",
       "  'kehrt': 708,\n",
       "  'ostteil': 709,\n",
       "  'ssi': 710,\n",
       "  'grenzboten': 711,\n",
       "  'neugewonnenen': 712,\n",
       "  'wieners': 713,\n",
       "  'herumsteigen': 714,\n",
       "  'adressaten': 715,\n",
       "  'schiuxt': 716,\n",
       "  'betÃ¤ubung': 717,\n",
       "  'dobromil': 718,\n",
       "  'erfreuen': 719,\n",
       "  'brunnen': 720,\n",
       "  'x&gt': 721,\n",
       "  'hi': 722,\n",
       "  'nationalehre': 723,\n",
       "  'abhold': 724,\n",
       "  'sonnenhitze': 725,\n",
       "  'rumler': 726,\n",
       "  'pont-Ã¤': 727,\n",
       "  'falkenhayn': 728,\n",
       "  'aufzuhebenden': 729,\n",
       "  'monarchistischer': 730,\n",
       "  'kriegsmaterial': 731,\n",
       "  'freihandelssystem': 732,\n",
       "  'zusammengeflickten': 733,\n",
       "  'gerichte': 734,\n",
       "  'mignet': 735,\n",
       "  'rinnt': 736,\n",
       "  'erlassung': 737,\n",
       "  'rechtlich': 738,\n",
       "  'auswendig': 739,\n",
       "  'hoffte': 740,\n",
       "  'hoffmann': 741,\n",
       "  'kowelâ€”nowno': 742,\n",
       "  'barbarei': 743,\n",
       "  'kosten': 744,\n",
       "  'weniger': 745,\n",
       "  'schuÃŸ': 746,\n",
       "  'fortzuschreiten': 747,\n",
       "  'seit': 748,\n",
       "  'leeren': 749,\n",
       "  'daran': 750,\n",
       "  'gemÃ¼thliches': 751,\n",
       "  'aufgedrungen': 752,\n",
       "  'kÃ¶nnte': 753,\n",
       "  'rzeszow': 754,\n",
       "  'trepow': 755,\n",
       "  'pilica': 756,\n",
       "  'gekommenen': 757,\n",
       "  'lomza': 758,\n",
       "  'emporblÃ¼hen': 759,\n",
       "  'geredet': 760,\n",
       "  'conversationsblatt': 761,\n",
       "  'verser': 762,\n",
       "  'recension': 763,\n",
       "  'angriffes': 764,\n",
       "  'ertrag': 765,\n",
       "  'sang': 766,\n",
       "  'angefacht': 767,\n",
       "  'zweimal': 768,\n",
       "  'ersparniÃŸanstalt': 769,\n",
       "  'petitionen': 770,\n",
       "  'grenzb-nen': 771,\n",
       "  'kapitalisten': 772,\n",
       "  'reciprocitÃ¤t': 773,\n",
       "  'ungleich': 774,\n",
       "  'begegnung': 775,\n",
       "  'zollvereinsstaaten': 776,\n",
       "  'wegwerfen': 777,\n",
       "  'malheur': 778,\n",
       "  'chefs': 779,\n",
       "  'avocourt': 780,\n",
       "  'losgesprochen': 781,\n",
       "  'weiÃŸ': 782,\n",
       "  'leitenden': 783,\n",
       "  'hinausgeworfen': 784,\n",
       "  'schlappe': 785,\n",
       "  'zurzeichnnng': 786,\n",
       "  'hallt': 787,\n",
       "  'kolonien': 788,\n",
       "  'unerquickliches': 789,\n",
       "  'soissons': 790,\n",
       "  'gefÃ¶rderte': 791,\n",
       "  'capitolium': 792,\n",
       "  'entferntesten': 793,\n",
       "  'weiter': 794,\n",
       "  'musikalischen': 795,\n",
       "  'tact': 796,\n",
       "  'seltsam': 797,\n",
       "  'einbalsamirt': 798,\n",
       "  'waffe': 799,\n",
       "  'frack': 800,\n",
       "  'vorschritten': 801,\n",
       "  'acccntuation': 802,\n",
       "  'constitutioncller': 803,\n",
       "  'zugÃ¤nglich': 804,\n",
       "  'sonderbarkeiten': 805,\n",
       "  'jude': 806,\n",
       "  'entweichungsplanes': 807,\n",
       "  'hochmuth': 808,\n",
       "  'bauernsohn': 809,\n",
       "  'windmÃ¼hlen': 810,\n",
       "  'verlautet': 811,\n",
       "  'auswahl': 812,\n",
       "  'gefalle': 813,\n",
       "  'gewerbeverein': 814,\n",
       "  'prozesses': 815,\n",
       "  'obrigkeit': 816,\n",
       "  'anfangs': 817,\n",
       "  'vorsprang': 818,\n",
       "  'jenieÃŸt': 819,\n",
       "  'johannes': 820,\n",
       "  'nachgesehen': 821,\n",
       "  'gehÃ¶ftes': 822,\n",
       "  'schlagende': 823,\n",
       "  'bergarbeiter': 824,\n",
       "  'gÃ¼': 825,\n",
       "  'dz': 826,\n",
       "  'aollwache': 827,\n",
       "  'auferlegung': 828,\n",
       "  'eingehÃ¼llt': 829,\n",
       "  'seeprovinzen': 830,\n",
       "  'tschorokflusses': 831,\n",
       "  'ausjenommen': 832,\n",
       "  'wohlgeordneten': 833,\n",
       "  'is-ni': 834,\n",
       "  'hohenbnrn': 835,\n",
       "  'fruchtbare': 836,\n",
       "  'anforderung': 837,\n",
       "  'ankÃ¼ndigung': 838,\n",
       "  'nahrungssorgen': 839,\n",
       "  \"muchar's\": 840,\n",
       "  'tuchsabrikant': 841,\n",
       "  'strengste': 842,\n",
       "  'vielbeschrienen': 843,\n",
       "  'oack': 844,\n",
       "  'verschlieÃŸen': 845,\n",
       "  'unbedingter': 846,\n",
       "  'schnelle': 847,\n",
       "  'lusitcmia': 848,\n",
       "  'romcigna': 849,\n",
       "  'compromittiren': 850,\n",
       "  'bescheid': 851,\n",
       "  'dahlbergs': 852,\n",
       "  'ausgestatteten': 853,\n",
       "  'leuchte': 854,\n",
       "  'weitklaffende': 855,\n",
       "  'esthland': 856,\n",
       "  'emil': 857,\n",
       "  'anschauungen': 858,\n",
       "  'kavalleriedivisionen': 859,\n",
       "  'reichskanzlers': 860,\n",
       "  'cesar': 861,\n",
       "  'dÃ¼rer': 862,\n",
       "  'unglÃ¼cks': 863,\n",
       "  'eingerichtet': 864,\n",
       "  'bestÃ¤tigte': 865,\n",
       "  'steuernachlaÃŸ': 866,\n",
       "  'verfertigung': 867,\n",
       "  'verbannten': 868,\n",
       "  'unglaubliche': 869,\n",
       "  'alap': 870,\n",
       "  'wÃ¼rdigte': 871,\n",
       "  'todt': 872,\n",
       "  'Ã¤rmern': 873,\n",
       "  'umgiebt': 874,\n",
       "  'diplomatisch': 875,\n",
       "  'ruÃŸ': 876,\n",
       "  'collectio': 877,\n",
       "  'zmigrod': 878,\n",
       "  'ehrenbezeugung': 879,\n",
       "  'plutarch': 880,\n",
       "  'hausthÃ¼re': 881,\n",
       "  'charakterschwÃ¤chen': 882,\n",
       "  'intelligenzblatt': 883,\n",
       "  'ver&gt': 884,\n",
       "  'schÃ¼tze': 885,\n",
       "  'theiÃŸ': 886,\n",
       "  'sieges': 887,\n",
       "  'mwirtlich': 888,\n",
       "  'erlassene': 889,\n",
       "  'musikgeschichts': 890,\n",
       "  'bedckovich': 891,\n",
       "  'aisneufer': 892,\n",
       "  'ins': 893,\n",
       "  'einpauken': 894,\n",
       "  'vorurtheile': 895,\n",
       "  'eingesendeten': 896,\n",
       "  'landstraÃŸe': 897,\n",
       "  'muthigen': 898,\n",
       "  'lÃ¶we': 899,\n",
       "  'frÃ¼h': 900,\n",
       "  'commandirende': 901,\n",
       "  'urtheilbefÃ¤higte': 902,\n",
       "  'lockern': 903,\n",
       "  'donnernde': 904,\n",
       "  'proletariats': 905,\n",
       "  'geldmacht': 906,\n",
       "  'tÃ¶nt': 907,\n",
       "  'zufalls': 908,\n",
       "  'erledigten': 909,\n",
       "  'abzuschrecken': 910,\n",
       "  'schul': 911,\n",
       "  'grelle': 912,\n",
       "  'eigene': 913,\n",
       "  'gardetruppen': 914,\n",
       "  'gefÃ¼hrt': 915,\n",
       "  'cabinets-censur': 916,\n",
       "  'vaterstÃ¤dtisches': 917,\n",
       "  'harmlasen': 918,\n",
       "  'geeilten': 919,\n",
       "  'geistesarme': 920,\n",
       "  'festesten': 921,\n",
       "  'ausspricht': 922,\n",
       "  'Ã¼berwiegend': 923,\n",
       "  'reminiscenzen': 924,\n",
       "  'ludwigs': 925,\n",
       "  'predigt': 926,\n",
       "  'monarchen': 927,\n",
       "  'zeitgedichte': 928,\n",
       "  'hinstellend': 929,\n",
       "  'versÃ¤umnisse': 930,\n",
       "  'vicekanzlers': 931,\n",
       "  'signor': 932,\n",
       "  'juliuswoche': 933,\n",
       "  'tenedos': 934,\n",
       "  'carl': 935,\n",
       "  'sammten': 936,\n",
       "  'fcstlandssperre': 937,\n",
       "  'diese': 938,\n",
       "  'verlacht': 939,\n",
       "  'vertrÃ¤ge': 940,\n",
       "  'leis': 941,\n",
       "  'iÂ»r': 942,\n",
       "  'gefahr': 943,\n",
       "  'verbÃ¼ndeten': 944,\n",
       "  'angesprochen': 945,\n",
       "  'federfuchser': 946,\n",
       "  'concessionirung': 947,\n",
       "  'Ã¼bersiedeln': 948,\n",
       "  'bepurpurtcn': 949,\n",
       "  'schulen': 950,\n",
       "  'schreiern': 951,\n",
       "  'ergaben': 952,\n",
       "  'christliches': 953,\n",
       "  'lilten': 954,\n",
       "  'geschwollene': 955,\n",
       "  'grenzzoll': 956,\n",
       "  'regeneriren': 957,\n",
       "  'volksklassen': 958,\n",
       "  'her-uigeber': 959,\n",
       "  'staatsregierung': 960,\n",
       "  'verlagsartikel': 961,\n",
       "  'wagte': 962,\n",
       "  'geschwaderflug': 963,\n",
       "  'hinzieht': 964,\n",
       "  'kunstwerkes': 965,\n",
       "  'rechtem': 966,\n",
       "  'liebhabertheater': 967,\n",
       "  'menil': 968,\n",
       "  'unvollkommenheit': 969,\n",
       "  'parteiorganisation': 970,\n",
       "  'buchhalter': 971,\n",
       "  'kÂ«vauda': 972,\n",
       "  'freut': 973,\n",
       "  'politischen': 974,\n",
       "  'geboten': 975,\n",
       "  'unzugÃ¤nglich': 976,\n",
       "  'nachahmung': 977,\n",
       "  'jahrgang': 978,\n",
       "  'brander': 979,\n",
       "  'marr': 980,\n",
       "  'ungemein': 981,\n",
       "  'ungewÃ¶hnliche': 982,\n",
       "  'geschÃ¼tzsront': 983,\n",
       "  'dramatisches': 984,\n",
       "  'feldzuge': 985,\n",
       "  'lÃ¼ckenhaften': 986,\n",
       "  'literatenwesen': 987,\n",
       "  'erfolgen': 988,\n",
       "  'wornach': 989,\n",
       "  'erwerben': 990,\n",
       "  'renzboten': 991,\n",
       "  'preÃŸ': 992,\n",
       "  'hÃ¼bschen': 993,\n",
       "  'friedrichshafen': 994,\n",
       "  'setzte': 995,\n",
       "  'kes': 996,\n",
       "  'tÂ»s': 997,\n",
       "  'dargeboten': 998,\n",
       "  'curiÃ¶ses': 999,\n",
       "  'la': 1000,\n",
       "  ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix, document_ids, type_ids = preprocessing.create_document_term_matrix(tokenized_corpus,\n",
    "                                                                                         meta['title'],\n",
    "                                                                                         large_corpus=True)\n",
    "(document_term_matrix, document_ids, type_ids)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Small corpus matrix\n",
    "\n",
    "Otherwise, use the document-term matrix desigend for small corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>die</th>\n",
       "      <th>der</th>\n",
       "      <th>und</th>\n",
       "      <th>in</th>\n",
       "      <th>den</th>\n",
       "      <th>von</th>\n",
       "      <th>zu</th>\n",
       "      <th>das</th>\n",
       "      <th>des</th>\n",
       "      <th>nicht</th>\n",
       "      <th>...</th>\n",
       "      <th>weitlinge</th>\n",
       "      <th>weitschichtige</th>\n",
       "      <th>welker</th>\n",
       "      <th>welscher</th>\n",
       "      <th>werthschÃ¤tzung</th>\n",
       "      <th>wesentlicher</th>\n",
       "      <th>wichtigeren</th>\n",
       "      <th>widerliche</th>\n",
       "      <th>widersetzlichen</th>\n",
       "      <th>gasfrage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tagebuch_56</th>\n",
       "      <td>90.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_94</th>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tagebuch_51</th>\n",
       "      <td>226.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_94</th>\n",
       "      <td>39.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kriegstagebuch_37</th>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     die    der    und     in   den   von    zu   das   des  \\\n",
       "Tagebuch_56         90.0   92.0   84.0   70.0  30.0  26.0  25.0  16.0  25.0   \n",
       "Kriegstagebuch_94   11.0   32.0   24.0   12.0   8.0  17.0   0.0   3.0   5.0   \n",
       "Tagebuch_51        226.0  177.0  188.0  111.0  73.0  62.0  93.0  60.0  35.0   \n",
       "Kriegstagebuch_94   39.0   48.0   34.0   28.0  15.0  25.0   4.0   5.0  11.0   \n",
       "Kriegstagebuch_37   40.0   34.0   15.0   17.0  10.0  19.0   5.0   6.0  18.0   \n",
       "\n",
       "                   nicht  ...  weitlinge  weitschichtige  welker  welscher  \\\n",
       "Tagebuch_56         23.0  ...        0.0             0.0     0.0       0.0   \n",
       "Kriegstagebuch_94    1.0  ...        0.0             0.0     0.0       0.0   \n",
       "Tagebuch_51         78.0  ...        0.0             0.0     0.0       0.0   \n",
       "Kriegstagebuch_94    3.0  ...        0.0             0.0     0.0       0.0   \n",
       "Kriegstagebuch_37    3.0  ...        0.0             0.0     0.0       0.0   \n",
       "\n",
       "                   werthschÃ¤tzung  wesentlicher  wichtigeren  widerliche  \\\n",
       "Tagebuch_56                   0.0           0.0          0.0         0.0   \n",
       "Kriegstagebuch_94             0.0           0.0          0.0         0.0   \n",
       "Tagebuch_51                   0.0           0.0          0.0         0.0   \n",
       "Kriegstagebuch_94             0.0           0.0          0.0         0.0   \n",
       "Kriegstagebuch_37             0.0           0.0          0.0         0.0   \n",
       "\n",
       "                   widersetzlichen  gasfrage  \n",
       "Tagebuch_56                    0.0       0.0  \n",
       "Kriegstagebuch_94              0.0       0.0  \n",
       "Tagebuch_51                    0.0       0.0  \n",
       "Kriegstagebuch_94              0.0       0.0  \n",
       "Kriegstagebuch_37              0.0       0.0  \n",
       "\n",
       "[5 rows x 24451 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix = preprocessing.create_document_term_matrix(tokenized_corpus, meta['title'])\n",
    "document_term_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.5. Feature removal\n",
    "\n",
    "*Stopwords* (also known as *most frequent tokens*) and *hapax legomena* are harmful for LDA and have to be removed from the corpus or the document-term matrix respectively. In this example, the 50 most frequent tokens will be categorized as stopwords.\n",
    "\n",
    "**Hint**: Be careful with removing most frequent tokens, you might remove tokens quite important for LDA. Anyway, to gain better results, it is highly recommended to use an external stopwords list.\n",
    "\n",
    "In this notebook, we combine the 50 most frequent tokens, hapax legomena and an external stopwordslist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the 100 most frequent words\n",
    "\n",
    "If you have chosen the large corpus model, you will have to add `type_ids` to the function `preprocessing.list_mfw()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = preprocessing.list_mfw(document_term_matrix, most_frequent_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the five most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die', 'der', 'und', 'in', 'den']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List hapax legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of types in corpus: 24451\n",
      "Total number of hapax legomena: 19757\n"
     ]
    }
   ],
   "source": [
    "hapax_legomena = preprocessing.find_hapax_legomena(document_term_matrix)\n",
    "print(\"Total number of types in corpus:\", document_term_matrix.shape[1])\n",
    "print(\"Total number of hapax legomena:\", len(hapax_legomena))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Use external stopwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stopwordlist = Path('data', 'stopwords', 'de.txt')\n",
    "external_stopwords = [line.strip() for line in path_to_stopwordlist.open('r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine lists and remove content from `tokenized_corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = stopwords + hapax_legomena + external_stopwords\n",
    "clean_tokenized_corpus = list(preprocessing.remove_features(features, tokenized_corpus=tokenized_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to MALLET folder \n",
    "\n",
    "Now we must tell the library where to find the local instance of MALLET. If you managed to install MALLET, it is sufficient set `path_to_mallet = 'mallet'`, if you store MALLET in a local folder, you have to specify the path to the binary explictly (e.g. `path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'`).\n",
    "\n",
    "**Whitespaces are not allowed in the path!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mallet = 'C:/Users/acer/Documents/TM_Schuchardt/mallet-2.0.8/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Create `Mallet` object\n",
    "\n",
    "Finally, we can instance the `Mallet` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mallet = utils.Mallet(path_to_mallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `Mallet` has a method `import_tokenized_corpus()` to create a specific corpus file for MALLET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 193] %1 ist keine zulÃ¤ssige Win32-Anwendung",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-40900ba2535d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmallet_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMallet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_tokenized_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_tokenized_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dariah_topics\\utils.py\u001b[0m in \u001b[0;36mimport_tokenized_corpus\u001b[1;34m(self, tokenized_corpus, document_labels, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mcorpus_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'corpus.mallet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mpostprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tokenized_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_mallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'import-dir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0m_check_mallet_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'corpus.mallet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dariah_topics\\utils.py\u001b[0m in \u001b[0;36mcall_mallet\u001b[1;34m(self, command, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mcommunicate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcall_commandline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommunicate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimport_tokenized_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenized_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dariah_topics\\utils.py\u001b[0m in \u001b[0;36mcall_commandline\u001b[1;34m(cmd, stdin, stdout, stderr, communicate, logfile)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calling the command-line: {0} ...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[0mdecoded_stderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 193] %1 ist keine zulÃ¤ssige Win32-Anwendung"
     ]
    }
   ],
   "source": [
    "mallet_corpus = Mallet.import_tokenized_corpus(clean_tokenized_corpus, meta['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, `Mallet` has the method `train_topics()` to create and train the LDA model. To create a LDA model, there have to be specified a couple of parameters.\n",
    "\n",
    "But first, if you are curious about any library, module, class or function, try `help()`. This can be very useful, because (at least in a well documented library) explanations of use and parameters will be printed. We're interested in the function `Mallet.train_topics()` in the module `dariah_topics.mallet`, so let's try:\n",
    "\n",
    "```\n",
    "help(mallet.Mallet)\n",
    "```\n",
    "\n",
    "This will print something like this (in fact even more):\n",
    "\n",
    "```\n",
    "Help on method train_topics in module dariah_topics.mallet:\n",
    "\n",
    "train_topics(mallet_binary, **kwargs) method of dariah_topics.mallet.Mallet instance\n",
    "    Args:\n",
    "        input_model (str): Absolute path to the binary topic model created by `output_model`.\n",
    "        output_model (str): Write a serialized MALLET topic trainer object.\n",
    "            This type of output is appropriate for pausing and restarting training,\n",
    "            but does not produce data that can easily be analyzed.\n",
    "        output_topic_keys (str): Write the top words for each topic and any\n",
    "            Dirichlet parameters to file.\n",
    "        topic_word_weights_file (str): Write unnormalized weights for every\n",
    "            topic and word type.\n",
    "        word_topic_counts_file (str): Write a sparse representation of topic-word\n",
    "            assignments. By default this is null, indicating that no file will\n",
    "            be written.\n",
    "        output_doc_topics (str): Write the topic proportions per document, at\n",
    "            the end of the iterations.\n",
    "        num_topics (int): Number of topics. Defaults to 10.\n",
    "        num_top_words (int): Number of keywords for each topic. Defaults to 10.\n",
    "        num_interations (int): Number of iterations. Defaults to 1000.\n",
    "        num_threads (int): Number of threads for parallel training.  Defaults to 1.\n",
    "        num_icm_iterations (int): Number of iterations of iterated conditional\n",
    "            modes (topic maximization).  Defaults to 0.\n",
    "        no_inference (bool): Load a saved model and create a report. Equivalent\n",
    "            to `num_iterations = 0`. Defaults to False.\n",
    "        random_seed (int): Random seed for the Gibbs sampler. Defaults to 0.\n",
    "        optimize_interval (int): Number of iterations between reestimating\n",
    "            dirichlet hyperparameters. Defaults to 0.\n",
    "        optimize_burn_in (int): Number of iterations to run before first\n",
    "            estimating dirichlet hyperparameters. Defaults to 200.\n",
    "        use_symmetric_alpha (bool): Only optimize the concentration parameter of\n",
    "            the prior over document-topic distributions. This may reduce the\n",
    "            number of very small, poorly estimated topics, but may disperse common\n",
    "            words over several topics. Defaults to False.\n",
    "        alpha (float): Sum over topics of smoothing over doc-topic distributions.\n",
    "            alpha_k = [this value] / [num topics]. Defaults to 5.0.\n",
    "        beta (float): Smoothing parameter for each topic-word. Defaults to 0.01.\n",
    "```\n",
    "\n",
    "So, now you know how to define the number of topics and the number of sampling iterations as well. A higher number of iterations will probably yield a better model, but also increases processing time. `alpha` and `beta` are so-called *hyperparameters*. They influence the model's performance, so feel free to play around with them. In the present example, we will leave the default values. Furthermore, there exist various methods for hyperparameter optimization, e.g. gridsearch or Gaussian optimization.\n",
    "\n",
    "**Warning: This step can take quite a while!** Meaning something between some seconds and some hours depending on corpus size and the number of iterations. Our example corpus should be done within a minute or two at `num_iterations=1000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create an output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Path('data', 'mallet_output')\n",
    "\n",
    "if not output.exists():\n",
    "    output.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Mallet.train_topics(mallet_corpus,\n",
    "                    output_topic_keys=str(Path(output, 'topic_keys.txt')),\n",
    "                    output_doc_topics=str(Path(output, 'doc_topics.txt')),\n",
    "                    num_topics=10,\n",
    "                    num_iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about MALLET's logging, have a look at the file `mallet.log`, which should have been created in the same directory as your notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create document-topic matrix\n",
    "\n",
    "The generated model object can now be translated into a human-readable document-topic matrix (that is a actually a pandas data frame) that constitutes our principle exchange format for topic modeling results. For generating the matrix from a Gensim model, we can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = postprocessing.show_topics(topic_keys_file=str(Path(output, 'topic_keys.txt')))\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each topic has a certain probability for each document in the corpus (have a look at the cell below). This probability distributions are visualized in an interactive **heatmap** (the darker the color, the higher the probability) which displays the kind of information\n",
    "                that is presumably most useful to literary scholars. Going beyond pure exploration, this visualization can be used to show thematic developments over a set of texts as well as a single text, akin to a dynamic topic model. What might become\n",
    "                apparent here, is that some topics correlate highly with a specific author or group of authors, while other topics correlate highly with a specific text or group of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = postprocessing.show_document_topics(topics=topics,\n",
    "                                                      doc_topics_file=str(Path(output, 'doc_topics.txt')))\n",
    "document_topics[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics over all documents\n",
    "\n",
    "The distribution of topics over all documents can now be visualized in an interactive heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDocumentTopics = visualization.PlotDocumentTopics(document_topics)\n",
    "show(PlotDocumentTopics.interactive_heatmap(), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a static heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_heatmap = PlotDocumentTopics.static_heatmap()\n",
    "static_heatmap.show()"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
